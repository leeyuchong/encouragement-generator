{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "f2f75a0ebfdd6e2b92c174594ae097d40e0e557cdf4b7d9b70ffaf179ea13763"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import csv"
   ]
  },
  {
   "source": [
    "<h1>Text preprocessing</h1>\n",
    "Code adapted from: https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num samples posts:  965\nNum samples comments output 965\nNum samples comments input 965\nwhen we started dating 5 years ago , i told her how much i wanted children . she said she could want kids with me . apparently , she is changed her mind and does not want kids at all now , and i just do not think i could be fulfilled without children . i do not want to lose her and our dog . i just feel so bad right now .\nim really sorry to hear that . may i ask why she doesnt want children ? or perhaps ask her why if you havent already . is she open to adopting maybe ? <END>\n<START> im really sorry to hear that . may i ask why she doesnt want children ? or perhaps ask her why if you havent already . is she open to adopting maybe ?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.replace(\"i'm\", \"i am\")\n",
    "    # text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = text.replace(\"he's\", \"he is\")\n",
    "    text = text.replace(\"she's\", \"she is\")\n",
    "    text = text.replace(\"it's\", \"it is\")\n",
    "    text = text.replace(\"what's\", \"that is\")\n",
    "    text = text.replace(\"that's\", \"that is\")\n",
    "    text = text.replace(\"where's\", \"where is\")\n",
    "    text = text.replace(\"how's\", \"how is\")\n",
    "    text = text.replace(\"\\'ll\", \" will\")\n",
    "    text = text.replace(\"\\'re\", \" are\")\n",
    "    text = text.replace(\"\\'ve\", \" have\")\n",
    "    text = text.replace(\"\\'d\", \" would\")\n",
    "    text = text.replace(\"won't\", \"will not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    text = text.replace(\"n'\", \"ng\")\n",
    "    text = text.replace(\"'bout\", \"about\")\n",
    "    text = text.replace(\"'til\", \"until\")\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # if text != \":)\" or text != \":(\":\n",
    "    #     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "start_char = \"<START>\"\n",
    "end_char = \"<END>\"\n",
    "\n",
    "posts = []\n",
    "comments_output = []\n",
    "comments_input = []\n",
    "\n",
    "with open('../encouragement_comments.csv', 'r', newline='') as csv_file:\n",
    "    textReader = csv.reader(csv_file)\n",
    "    for row in textReader:\n",
    "        cleaned_text = clean_text(row[1])\n",
    "        posts.append(\" \".join(re.findall(r\"[\\w']+|[.,!?;]\", cleaned_text)))\n",
    "\n",
    "        cleaned_text = clean_text(row[0])\n",
    "        cleaned_text = re.findall(r\"[\\w']+|[.,!?;]\", cleaned_text)\n",
    "        cleaned_text = [ x.strip() for x in cleaned_text ]\n",
    "        cleaned_text = [start_char] + cleaned_text + [end_char]\n",
    "        comments_output.append(\" \".join(cleaned_text[1:]))\n",
    "        comments_input.append(\" \".join(cleaned_text[:-1]))\n",
    "print(\"Num samples posts: \", len(posts))\n",
    "print(\"Num samples comments output\", len(comments_output))\n",
    "print(\"Num samples comments input\", len(comments_input))\n",
    "print(posts[500])\n",
    "print(comments_output[500])\n",
    "print(comments_input[500])"
   ]
  },
  {
   "source": [
    "# Generate all the unique words in the data set\n",
    "all_words = set()\n",
    "all_text = posts + comments_output + comments_input\n",
    "for sentence in all_text:\n",
    "    for word in sentence.split():\n",
    "        all_words.add(word)\n",
    "print(\"Total number of unique words: \", len(all_words))\n",
    "\n",
    "for i, word in enumerate(all_words):\n",
    "    if i < 5:\n",
    "        print(word)\n",
    "    else: break"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of unique words:  8453\ngoooooooo\norders\nimmensely\nprovider\nsings\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total unique words:  8453\n",
      "Max post length:  3501\n",
      "Average post length:  202.76269430051815\n",
      "median_post_length:  134.0\n",
      "Max comment length:  923\n",
      "Average comment length:  95.30259067357512\n",
      "95.30259067357512\n",
      "median_comment_length:  66.0\n",
      "203\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from scipy import stats\n",
    "\n",
    "vocab_size = len(all_words)\n",
    "tokenizer = Tokenizer(num_words = vocab_size+1, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "word_to_index = tokenizer.word_index\n",
    "\n",
    "print(\"Total unique words: \", len(word_to_index))\n",
    "\n",
    "posts_sequence = tokenizer.texts_to_sequences(posts)\n",
    "MAX_POST_LEN = max(len(seq) for seq in posts_sequence)\n",
    "average_post_length = np.average([len(seq) for seq in posts_sequence])\n",
    "median_post_length = np.median([len(seq) for seq in posts_sequence])\n",
    "print(\"Max post length: \", MAX_POST_LEN)\n",
    "print(\"Average post length: \", average_post_length)\n",
    "print(\"median_post_length: \", median_post_length)\n",
    "\n",
    "comments_output_sequence = tokenizer.texts_to_sequences(comments_output)\n",
    "comments_input_sequence = tokenizer.texts_to_sequences(comments_input)\n",
    "MAX_COMMENT_LEN = max(len(seq) for seq in comments_output_sequence)\n",
    "average_comment_length = np.average([len(seq) for seq in comments_output_sequence])\n",
    "median_comment_length = np.median([len(seq) for seq in comments_output_sequence])\n",
    "print(\"Max comment length: \", MAX_COMMENT_LEN)\n",
    "print(\"Average comment length: \", average_comment_length)\n",
    "print(np.average([len(seq) for seq in comments_input_sequence]))\n",
    "print(\"median_comment_length: \", median_comment_length)\n",
    "\n",
    "AVG_POST_LEN = int(round(average_post_length))\n",
    "print(AVG_POST_LEN)\n",
    "AVG_COMMENT_LEN = int(round(average_comment_length))\n",
    "print(AVG_COMMENT_LEN)\n",
    "\n",
    "# index_to_word = dict()\n",
    "# for k, v in word_to_index.items():\n",
    "#     index_to_word[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[57, 2077, 6, 200, 17, 7, 3917, 36, 3196, 881, 36, 1252, 27, 125, 3144, 109, 51, 1, 6, 33, 20, 9, 30, 99, 7, 341, 12, 35, 1209, 7, 299, 738, 169, 242, 6, 4225, 4, 20, 394, 155, 1, 6, 2975, 3, 6, 576, 3, 17, 7, 739, 11, 1429, 3, 5821, 418, 3, 2730, 3, 5822, 3, 962, 3, 5, 273, 20, 7, 2819, 125, 64, 4, 4028, 1, 42, 16, 2055, 787, 112, 238, 2, 92, 55, 74, 15, 16, 72, 881, 32, 10, 9, 335, 4, 196, 637, 27, 7, 3082, 11, 991, 10, 6, 33, 3292, 4, 494, 129, 5823, 881, 504, 1, 83, 11, 706, 94, 135, 4226, 35, 719, 86, 36, 1634, 1427, 36, 2843, 36, 1563, 1, 2, 130, 46, 95, 55, 2, 17, 1714, 228, 4226, 29, 13, 5824, 516, 27, 7, 70, 5825, 4, 84, 19, 241, 297, 568, 1, 5, 20, 14, 22, 326, 4, 494, 504, 27, 8, 4227, 10, 21, 200, 14, 22, 1408, 2375, 3, 18, 68, 32, 7, 45, 55, 1, 1, 1, 2, 17, 344, 246, 2, 128, 21, 5826, 8, 3293, 200, 216, 8, 685, 15, 287, 102, 36, 923, 78, 2374, 3, 497, 19], [57, 55, 2, 32, 15, 226, 3, 2, 383, 4, 535, 15, 1028, 953, 1281, 639, 27, 5834, 1, 617, 1263, 5835, 3, 128, 9, 7, 118, 34, 6, 24, 15, 76, 11, 7, 96, 1], [57], [57, 22, 224, 3, 16, 219, 6, 552, 12, 650, 7, 115, 1415, 1, 5836, 5837, 2534, 5838, 518, 5839, 1, 617, 5840, 5841, 2195, 22, 224, 1, 6, 24, 14, 250, 1, 52, 24, 40, 650, 8, 188, 1415, 91, 154, 91, 3, 5, 9, 174, 37, 604, 27, 45, 1, 6, 1131, 4, 606, 10, 40, 165, 487, 435, 24, 26, 435, 1, 67, 20, 14, 391, 155, 3, 5, 67, 24, 14, 1671, 623, 1, 6, 33, 37, 41, 11, 582, 833, 165, 435, 1, 6, 33, 17, 1707, 5, 1183, 833, 165, 435, 1, 6, 33, 5842, 39, 93, 1451, 1, 6, 33, 111, 353, 1], [57, 149, 31, 22, 347, 15, 8, 211, 1, 34, 9, 12, 14, 347, 3, 9, 12, 14, 8, 211, 1]]\n[[2077, 6, 200, 17, 7, 3917, 36, 3196, 881, 36, 1252, 27, 125, 3144, 109, 51, 1, 6, 33, 20, 9, 30, 99, 7, 341, 12, 35, 1209, 7, 299, 738, 169, 242, 6, 4225, 4, 20, 394, 155, 1, 6, 2975, 3, 6, 576, 3, 17, 7, 739, 11, 1429, 3, 5821, 418, 3, 2730, 3, 5822, 3, 962, 3, 5, 273, 20, 7, 2819, 125, 64, 4, 4028, 1, 42, 16, 2055, 787, 112, 238, 2, 92, 55, 74, 15, 16, 72, 881, 32, 10, 9, 335, 4, 196, 637, 27, 7, 3082, 11, 991, 10, 6, 33, 3292, 4, 494, 129, 5823, 881, 504, 1, 83, 11, 706, 94, 135, 4226, 35, 719, 86, 36, 1634, 1427, 36, 2843, 36, 1563, 1, 2, 130, 46, 95, 55, 2, 17, 1714, 228, 4226, 29, 13, 5824, 516, 27, 7, 70, 5825, 4, 84, 19, 241, 297, 568, 1, 5, 20, 14, 22, 326, 4, 494, 504, 27, 8, 4227, 10, 21, 200, 14, 22, 1408, 2375, 3, 18, 68, 32, 7, 45, 55, 1, 1, 1, 2, 17, 344, 246, 2, 128, 21, 5826, 8, 3293, 200, 216, 8, 685, 15, 287, 102, 36, 923, 78, 2374, 3, 497, 19, 56], [55, 2, 32, 15, 226, 3, 2, 383, 4, 535, 15, 1028, 953, 1281, 639, 27, 5834, 1, 617, 1263, 5835, 3, 128, 9, 7, 118, 34, 6, 24, 15, 76, 11, 7, 96, 1, 56], [56], [22, 224, 3, 16, 219, 6, 552, 12, 650, 7, 115, 1415, 1, 5836, 5837, 2534, 5838, 518, 5839, 1, 617, 5840, 5841, 2195, 22, 224, 1, 6, 24, 14, 250, 1, 52, 24, 40, 650, 8, 188, 1415, 91, 154, 91, 3, 5, 9, 174, 37, 604, 27, 45, 1, 6, 1131, 4, 606, 10, 40, 165, 487, 435, 24, 26, 435, 1, 67, 20, 14, 391, 155, 3, 5, 67, 24, 14, 1671, 623, 1, 6, 33, 37, 41, 11, 582, 833, 165, 435, 1, 6, 33, 17, 1707, 5, 1183, 833, 165, 435, 1, 6, 33, 5842, 39, 93, 1451, 1, 6, 33, 111, 353, 1, 56], [149, 31, 22, 347, 15, 8, 211, 1, 34, 9, 12, 14, 347, 3, 9, 12, 14, 8, 211, 1, 56]]\n81.9139896373057\n81.9139896373057\n203\n203\n"
     ]
    }
   ],
   "source": [
    "# Truncate the output to the average length of a comment (203)\n",
    "truncated = []\n",
    "for comment in comments_input_sequence:\n",
    "    if len(comment) > 203:\n",
    "        truncated.append(comment[:203])\n",
    "    else:\n",
    "        truncated.append(comment)\n",
    "comments_input_sequence = truncated.copy()\n",
    "print(comments_input_sequence[:5])\n",
    "truncated = []\n",
    "\n",
    "for comment in comments_output_sequence:\n",
    "    if len(comment) > 203:\n",
    "        truncated.append(comment[:202] + [word_to_index['<END>']])\n",
    "    else:\n",
    "        truncated.append(comment)\n",
    "comments_output_sequence = truncated.copy()\n",
    "print(comments_output_sequence[:5])\n",
    "\n",
    "print(np.average([len(seq) for seq in comments_output_sequence]))\n",
    "print(np.average([len(seq) for seq in comments_input_sequence]))\n",
    "print(max([len(seq) for seq in comments_output_sequence]))\n",
    "print(max([len(seq) for seq in comments_input_sequence]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "padded_post_sequences.shape (965, 203)\npadded_post_sequences[500] [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0   55   52  260  874  377  133  259    3    2  229   77\n   73  101    2  206  918    1   71  220   71   97   62  477   27   19\n    1 4181    3   71   12  736   77  300    5  174   14   62  477   39\n   40   51    3    5    2   26   20   14   83    2   97   22 2074  306\n  918    1    2   20   14   62    4  464   77    5  187  769    1    2\n   26   46   28  151   81   51    1]\n52\n(965, 95)\n[  57   44   50  193    4  283   10    1  200    2  216  244   71  310\n   62  918   59   36  873  216   77  244   34    6  836  264    1   12\n   71  482    4 6772  153   59    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0]\n57\n"
     ]
    }
   ],
   "source": [
    "#PADDING\n",
    "\n",
    "padded_post_sequences = pad_sequences(posts_sequence, maxlen=AVG_POST_LEN, truncating='post')\n",
    "padded_comment_input_sequences = pad_sequences(comments_input_sequence, maxlen=AVG_COMMENT_LEN, padding='post')\n",
    "padded_comment_output_sequences = pad_sequences(comments_output_sequence, maxlen=AVG_COMMENT_LEN, padding='post')\n",
    "print(\"padded_post_sequences.shape\", padded_post_sequences.shape)\n",
    "print(\"padded_post_sequences[500]\", padded_post_sequences[500])\n",
    "print(word_to_index['we'])\n",
    "print(padded_comment_input_sequences.shape)\n",
    "print(padded_comment_input_sequences[500])\n",
    "print(word_to_index['<START>'])\n",
    "\n",
    "# encoder_sequences = tokenizer.texts_to_sequences(posts)\n",
    "# encoder_sequences_padded = pad_sequences(encoder_sequences, maxlen=max_source_length, dtype='int32', padding='post', truncating='post')\n",
    "# decoder_sequences = tokenizer.texts_to_sequences(comments)\n",
    "# decoder_sequences_padded = pad_sequences(decoder_sequences, maxlen=max_target_length, dtype='int32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Layer will consider the top 20000 words and will pad or truncate words to be 200 words long \n",
    "# vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "# text_ds = tf.data.Dataset.from_tensor_slices(comments).batch(128)\n",
    "# vectorizer.adapt(text_ds)\n",
    "# print(vectorizer.get_vocabulary()[:5])\n",
    "\n",
    "#dict mapping words to their indices\n",
    "# voc = vectorizer.get_vocabulary()\n",
    "# word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "embeddings_dictionary = {}\n",
    "with open('./glove6B/glove.6B.100d.txt', 'r') as glove:\n",
    "    for line in glove:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converted 7888 words (565 misses)\n[-0.17791   0.62675   0.4787   -0.55295  -0.84935  -0.070802 -0.34724\n  0.4628    0.12611  -0.24875   0.46881   0.083636  0.56065  -0.21931\n  0.015561 -0.55806  -0.20738   0.9123   -1.2034    0.30115   0.46676\n  0.483    -0.10204  -0.56799  -0.027126  0.40567  -0.14058  -0.55485\n  0.094588 -0.62213  -0.30343   0.60639   0.049799  0.22204   0.48549\n  0.17629  -0.090535  0.53705   0.2755   -0.78827  -0.70953  -0.16678\n  0.11206  -0.48491  -0.66644   0.083952  0.32885  -0.45851  -0.37208\n -1.5315    0.12994  -0.2409   -0.17219   1.374    -0.22313  -2.615\n  0.35201   0.33597   1.6117    0.92947  -0.37535   0.82034  -1.0677\n -0.45329   1.2332    0.23749   0.63523   0.82859  -0.1744   -0.5853\n  0.56339  -0.73094   0.30815  -1.0888    0.46139   0.045386 -0.17827\n -0.054054 -0.8831    0.033935  0.63083  -0.19741  -0.99051   0.20022\n -1.9266   -0.25884   0.10367  -0.34129  -0.93507  -0.54666  -0.40171\n -0.37783  -0.065771 -0.13836  -0.91872  -0.055635 -0.080557 -0.19526\n  0.20078   1.0953  ]\n(8454, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
    "for word, i in word_to_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "print(embeddings_dictionary[\"we\"])\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "source": [
    "<h1>Embedding Layer</h1>\n",
    "The size of the embedding layer is the size of the vector that represents each word. We usually match the size of the embedding layer output with the number of hidden layers in the LSTM cell. \n",
    "\n",
    "The size of the hidden layer is equal to the number nodes representing the signmoid, tanh and hidden state layer in the LSTM cell. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8454\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import Constant\n",
    "embedding_layer = Embedding(vocab_size+1, embedding_dim, embeddings_initializer=Constant(embedding_matrix), input_length=AVG_POST_LEN, trainable=False)\n",
    "print(vocab_size+1)\n",
    "# embedding_layer = Embedding(\n",
    "#     num_tokens,\n",
    "#     embedding_dim,\n",
    "#     embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "#     trainable=False,\n",
    "# )"
   ]
  },
  {
   "source": [
    "CREATING THE MODEL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(965, 95, 8454)\n"
     ]
    }
   ],
   "source": [
    "decoder_targets_one_hot = np.zeros((\n",
    "    len(posts),\n",
    "    AVG_COMMENT_LEN,\n",
    "    vocab_size+1\n",
    "    ), \n",
    "    dtype='float32' \n",
    ")\n",
    "print(decoder_targets_one_hot.shape)\n",
    "\n",
    "# # One-hot encoding of the output\n",
    "# num_samples = len(encoder_sequences)\n",
    "# decoder_output_data = np.zeros((num_samples, max_target_length, vocab_size), dtype='float32')\n",
    "for i, seqs in enumerate(padded_comment_output_sequences):\n",
    "    for j, seq in enumerate(seqs):\n",
    "        if j > 0:\n",
    "            decoder_targets_one_hot[i, j, seq] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the encoder\n",
    "\n",
    "latent_dim = 256 #LSTM_NODES = latent_dim. Either set to 256 or 50???\n",
    "\n",
    "encoder_inputs = Input(shape=(AVG_POST_LEN,))\n",
    "enc_emb =  embedding_layer(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the decoder\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(AVG_COMMENT_LEN,)) \n",
    "dec_emb_layer = Embedding(vocab_size+1, latent_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from the decoder LSTM\n",
    "\n",
    "decoder_dense = Dense(vocab_size+1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# import keras\n",
    "# import pydot as pyd\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# keras.utils.vis_utils.pydot = pyd\n",
    "\n",
    "# #Visualize Model\n",
    "\n",
    "# def visualize_model(model):\n",
    "#   return SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "# #create your model\n",
    "# #then call the function on your model\n",
    "# visualize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.fit(\n",
    "    x=[padded_post_sequences, padded_comment_input_sequences],\n",
    "    y=decoder_targets_one_hot,\n",
    "    batch_size=64,\n",
    "    epochs=20,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {v:k for k,v in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_encouragement(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = word_to_index['<START>']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_word[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '<END>' or len(decoded_sentence) > 203):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    i = np.random.choice(len(posts))\n",
    "    input_seq = padded_post_sequences[i]\n",
    "    generated_comment = give_encouragement(input_seq)\n",
    "    print('-')\n",
    "    print('Post: ', posts[i])\n",
    "    print('Generated Comment: ', generated_comment)"
   ]
  },
  {
   "source": [
    "Keras LSTM Architecture\n",
    "\n",
    "The input shape of the text data is ordered as follows: batch size, number of time steps, hidden size (size of the hidden layer)\n",
    "For each batch sample and each word in the number of time steps, there is a l500 length embedding word vector to "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('saved_weights.hdf5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}