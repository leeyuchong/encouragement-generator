{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "f2f75a0ebfdd6e2b92c174594ae097d40e0e557cdf4b7d9b70ffaf179ea13763"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import csv"
   ]
  },
  {
   "source": [
    "<h1>Text preprocessing</h1>\n",
    "Code adapted from: https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num samples posts:  965\nNum samples comments output 965\nNum samples comments input 965\nwhen we started dating 5 years ago , i told her how much i wanted children . she said she could want kids with me . apparently , she is changed her mind and does not want kids at all now , and i just do not think i could be fulfilled without children . i do not want to lose her and our dog . i just feel so bad right now .\nim really sorry to hear that . may i ask why she doesnt want children ? or perhaps ask her why if you havent already . is she open to adopting maybe ? <END>\n<START> im really sorry to hear that . may i ask why she doesnt want children ? or perhaps ask her why if you havent already . is she open to adopting maybe ?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.replace(\"i'm\", \"i am\")\n",
    "    # text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = text.replace(\"he's\", \"he is\")\n",
    "    text = text.replace(\"she's\", \"she is\")\n",
    "    text = text.replace(\"it's\", \"it is\")\n",
    "    text = text.replace(\"what's\", \"that is\")\n",
    "    text = text.replace(\"that's\", \"that is\")\n",
    "    text = text.replace(\"where's\", \"where is\")\n",
    "    text = text.replace(\"how's\", \"how is\")\n",
    "    text = text.replace(\"\\'ll\", \" will\")\n",
    "    text = text.replace(\"\\'re\", \" are\")\n",
    "    text = text.replace(\"\\'ve\", \" have\")\n",
    "    text = text.replace(\"\\'d\", \" would\")\n",
    "    text = text.replace(\"won't\", \"will not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    text = text.replace(\"n'\", \"ng\")\n",
    "    text = text.replace(\"'bout\", \"about\")\n",
    "    text = text.replace(\"'til\", \"until\")\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # if text != \":)\" or text != \":(\":\n",
    "    #     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "start_char = \"<START>\"\n",
    "end_char = \"<END>\"\n",
    "\n",
    "posts = []\n",
    "comments_output = []\n",
    "comments_input = []\n",
    "\n",
    "with open('../encouragement_comments.csv', 'r', newline='') as csv_file:\n",
    "    textReader = csv.reader(csv_file)\n",
    "    for row in textReader:\n",
    "        cleaned_text = clean_text(row[1])\n",
    "        posts.append(\" \".join(re.findall(r\"[\\w']+|[.,!?;]\", cleaned_text)))\n",
    "\n",
    "        cleaned_text = clean_text(row[0])\n",
    "        cleaned_text = re.findall(r\"[\\w']+|[.,!?;]\", cleaned_text)\n",
    "        cleaned_text = [ x.strip() for x in cleaned_text ]\n",
    "        cleaned_text = [start_char] + cleaned_text + [end_char]\n",
    "        comments_output.append(\" \".join(cleaned_text[1:]))\n",
    "        comments_input.append(\" \".join(cleaned_text[:-1]))\n",
    "print(\"Num samples posts: \", len(posts))\n",
    "print(\"Num samples comments output\", len(comments_output))\n",
    "print(\"Num samples comments input\", len(comments_input))\n",
    "print(posts[500])\n",
    "print(comments_output[500])\n",
    "print(comments_input[500])"
   ]
  },
  {
   "source": [
    "# Generate all the unique words in the data set\n",
    "all_words = set()\n",
    "all_text = posts + comments_output + comments_input\n",
    "for sentence in all_text:\n",
    "    for word in sentence.split():\n",
    "        all_words.add(word)\n",
    "print(\"Total number of unique words: \", len(all_words))\n",
    "\n",
    "for i, word in enumerate(all_words):\n",
    "    if i < 5:\n",
    "        print(word)\n",
    "    else: break"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of unique words:  8453\nwins\nrescue\nbases\naware\nrespecting\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total unique words:  8453\nMax post length:  3501\nAverage post length:  202.76269430051815\nmedian_post_length:  134.0\nMax comment length:  923\nAverage comment length:  95.30259067357512\n95.30259067357512\nmedian_comment_length:  66.0\n203\n95\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from scipy import stats\n",
    "\n",
    "vocab_size = len(all_words)\n",
    "tokenizer = Tokenizer(num_words = vocab_size+1, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "word_to_index = tokenizer.word_index\n",
    "index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "print(\"Total unique words: \", len(word_to_index))\n",
    "\n",
    "posts_sequence = tokenizer.texts_to_sequences(posts)\n",
    "MAX_POST_LEN = max(len(seq) for seq in posts_sequence)\n",
    "average_post_length = np.average([len(seq) for seq in posts_sequence])\n",
    "median_post_length = np.median([len(seq) for seq in posts_sequence])\n",
    "print(\"Max post length: \", MAX_POST_LEN)\n",
    "print(\"Average post length: \", average_post_length)\n",
    "print(\"median_post_length: \", median_post_length)\n",
    "\n",
    "comments_output_sequence = tokenizer.texts_to_sequences(comments_output)\n",
    "comments_input_sequence = tokenizer.texts_to_sequences(comments_input)\n",
    "MAX_COMMENT_LEN = max(len(seq) for seq in comments_output_sequence)\n",
    "average_comment_length = np.average([len(seq) for seq in comments_output_sequence])\n",
    "median_comment_length = np.median([len(seq) for seq in comments_output_sequence])\n",
    "print(\"Max comment length: \", MAX_COMMENT_LEN)\n",
    "print(\"Average comment length: \", average_comment_length)\n",
    "print(np.average([len(seq) for seq in comments_input_sequence]))\n",
    "print(\"median_comment_length: \", median_comment_length)\n",
    "\n",
    "AVG_POST_LEN = int(round(average_post_length))\n",
    "print(AVG_POST_LEN)\n",
    "AVG_COMMENT_LEN = int(round(average_comment_length))\n",
    "print(AVG_COMMENT_LEN)\n",
    "\n",
    "# index_to_word = dict()\n",
    "# for k, v in word_to_index.items():\n",
    "#     index_to_word[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[57, 2077, 6, 200, 17, 7, 3917, 36, 3196, 881, 36, 1252, 27, 125, 3144, 109, 51, 1, 6, 33, 20, 9, 30, 99, 7, 341, 12, 35, 1209, 7, 299, 738, 169, 242, 6, 4225, 4, 20, 394, 155, 1, 6, 2975, 3, 6, 576, 3, 17, 7, 739, 11, 1429, 3, 5821, 418, 3, 2730, 3, 5822, 3, 962, 3, 5, 273, 20, 7, 2819, 125, 64, 4, 4028, 1, 42, 16, 2055, 787, 112, 238, 2, 92, 55, 74, 15, 16, 72, 881, 32, 10, 9, 335, 4, 196, 637, 27, 7], [57, 55, 2, 32, 15, 226, 3, 2, 383, 4, 535, 15, 1028, 953, 1281, 639, 27, 5834, 1, 617, 1263, 5835, 3, 128, 9, 7, 118, 34, 6, 24, 15, 76, 11, 7, 96, 1], [57], [57, 22, 224, 3, 16, 219, 6, 552, 12, 650, 7, 115, 1415, 1, 5836, 5837, 2534, 5838, 518, 5839, 1, 617, 5840, 5841, 2195, 22, 224, 1, 6, 24, 14, 250, 1, 52, 24, 40, 650, 8, 188, 1415, 91, 154, 91, 3, 5, 9, 174, 37, 604, 27, 45, 1, 6, 1131, 4, 606, 10, 40, 165, 487, 435, 24, 26, 435, 1, 67, 20, 14, 391, 155, 3, 5, 67, 24, 14, 1671, 623, 1, 6, 33, 37, 41, 11, 582, 833, 165, 435, 1, 6, 33, 17, 1707, 5, 1183, 833], [57, 149, 31, 22, 347, 15, 8, 211, 1, 34, 9, 12, 14, 347, 3, 9, 12, 14, 8, 211, 1]]\n[[2077, 6, 200, 17, 7, 3917, 36, 3196, 881, 36, 1252, 27, 125, 3144, 109, 51, 1, 6, 33, 20, 9, 30, 99, 7, 341, 12, 35, 1209, 7, 299, 738, 169, 242, 6, 4225, 4, 20, 394, 155, 1, 6, 2975, 3, 6, 576, 3, 17, 7, 739, 11, 1429, 3, 5821, 418, 3, 2730, 3, 5822, 3, 962, 3, 5, 273, 20, 7, 2819, 125, 64, 4, 4028, 1, 42, 16, 2055, 787, 112, 238, 2, 92, 55, 74, 15, 16, 72, 881, 32, 10, 9, 335, 4, 196, 637, 27, 7, 56], [55, 2, 32, 15, 226, 3, 2, 383, 4, 535, 15, 1028, 953, 1281, 639, 27, 5834, 1, 617, 1263, 5835, 3, 128, 9, 7, 118, 34, 6, 24, 15, 76, 11, 7, 96, 1, 56], [56], [22, 224, 3, 16, 219, 6, 552, 12, 650, 7, 115, 1415, 1, 5836, 5837, 2534, 5838, 518, 5839, 1, 617, 5840, 5841, 2195, 22, 224, 1, 6, 24, 14, 250, 1, 52, 24, 40, 650, 8, 188, 1415, 91, 154, 91, 3, 5, 9, 174, 37, 604, 27, 45, 1, 6, 1131, 4, 606, 10, 40, 165, 487, 435, 24, 26, 435, 1, 67, 20, 14, 391, 155, 3, 5, 67, 24, 14, 1671, 623, 1, 6, 33, 37, 41, 11, 582, 833, 165, 435, 1, 6, 33, 17, 1707, 5, 1183, 833, 56], [149, 31, 22, 347, 15, 8, 211, 1, 34, 9, 12, 14, 347, 3, 9, 12, 14, 8, 211, 1, 56]]\n61.087046632124355\n61.087046632124355\n95\n95\n"
     ]
    }
   ],
   "source": [
    "# Truncate the output to the average length of a comment (203)\n",
    "truncated = []\n",
    "for comment in comments_input_sequence:\n",
    "    if len(comment) > 95:\n",
    "        truncated.append(comment[:95])\n",
    "    else:\n",
    "        truncated.append(comment)\n",
    "comments_input_sequence = truncated.copy()\n",
    "print(comments_input_sequence[:5])\n",
    "truncated = []\n",
    "\n",
    "for comment in comments_output_sequence:\n",
    "    if len(comment) > 95:\n",
    "        truncated.append(comment[:94] + [word_to_index['<END>']])\n",
    "    else:\n",
    "        truncated.append(comment)\n",
    "comments_output_sequence = truncated.copy()\n",
    "print(comments_output_sequence[:5])\n",
    "\n",
    "print(np.average([len(seq) for seq in comments_output_sequence]))\n",
    "print(np.average([len(seq) for seq in comments_input_sequence]))\n",
    "print(max([len(seq) for seq in comments_output_sequence]))\n",
    "print(max([len(seq) for seq in comments_input_sequence]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "padded_post_sequences.shape (965, 203)\npadded_post_sequences[500] [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0   55   52  260  874  377  133  259    3    2  229   77\n   73  101    2  206  918    1   71  220   71   97   62  477   27   19\n    1 4181    3   71   12  736   77  300    5  174   14   62  477   39\n   40   51    3    5    2   26   20   14   83    2   97   22 2074  306\n  918    1    2   20   14   62    4  464   77    5  187  769    1    2\n   26   46   28  151   81   51    1]\n52\n(965, 95)\n[  57   44   50  193    4  283   10    1  200    2  216  244   71  310\n   62  918   59   36  873  216   77  244   34    6  836  264    1   12\n   71  482    4 6772  153   59    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0]\n57\n"
     ]
    }
   ],
   "source": [
    "#PADDING\n",
    "\n",
    "padded_post_sequences = pad_sequences(posts_sequence, maxlen=AVG_POST_LEN, truncating='post')\n",
    "padded_comment_input_sequences = pad_sequences(comments_input_sequence, maxlen=AVG_COMMENT_LEN, padding='post')\n",
    "padded_comment_output_sequences = pad_sequences(comments_output_sequence, maxlen=AVG_COMMENT_LEN, padding='post')\n",
    "print(\"padded_post_sequences.shape\", padded_post_sequences.shape)\n",
    "print(\"padded_post_sequences[500]\", padded_post_sequences[500])\n",
    "print(word_to_index['we'])\n",
    "print(padded_comment_input_sequences.shape)\n",
    "print(padded_comment_input_sequences[500])\n",
    "print(word_to_index['<START>'])\n",
    "\n",
    "# encoder_sequences = tokenizer.texts_to_sequences(posts)\n",
    "# encoder_sequences_padded = pad_sequences(encoder_sequences, maxlen=max_source_length, dtype='int32', padding='post', truncating='post')\n",
    "# decoder_sequences = tokenizer.texts_to_sequences(comments)\n",
    "# decoder_sequences_padded = pad_sequences(decoder_sequences, maxlen=max_target_length, dtype='int32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer will consider the top 20000 words and will pad or truncate words to be 200 words long \n",
    "# vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "# text_ds = tf.data.Dataset.from_tensor_slices(comments).batch(128)\n",
    "# vectorizer.adapt(text_ds)\n",
    "# print(vectorizer.get_vocabulary()[:5])\n",
    "\n",
    "#dict mapping words to their indices\n",
    "# voc = vectorizer.get_vocabulary()\n",
    "# word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "embeddings_dictionary = {}\n",
    "with open('./glove6B/glove.6B.100d.txt', 'r') as glove:\n",
    "    for line in glove:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
    "for word, i in word_to_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "print(embeddings_dictionary[\"we\"])\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "source": [
    "<h1>Embedding Layer</h1>\n",
    "The size of the embedding layer is the size of the vector that represents each word. We usually match the size of the embedding layer output with the number of hidden layers in the LSTM cell. \n",
    "\n",
    "The size of the hidden layer is equal to the number nodes representing the signmoid, tanh and hidden state layer in the LSTM cell. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "# embedding_layer = Embedding(vocab_size+1, embedding_dim, embeddings_initializer=Constant(embedding_matrix),  trainable=False)\n",
    "print(vocab_size+1)\n",
    "embedding_layer = Embedding(vocab_size+1, embedding_dim, embeddings_initializer=Constant(embedding_matrix), input_length=AVG_POST_LEN, trainable=False)\n"
   ]
  },
  {
   "source": [
    "CREATING THE MODEL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets_one_hot = np.zeros((\n",
    "    len(posts),\n",
    "    AVG_COMMENT_LEN,\n",
    "    vocab_size+1\n",
    "    ), \n",
    "    dtype='float32' \n",
    ")\n",
    "print(decoder_targets_one_hot.shape)\n",
    "\n",
    "# # One-hot encoding of the output\n",
    "# num_samples = len(encoder_sequences)\n",
    "# decoder_output_data = np.zeros((num_samples, max_target_length, vocab_size), dtype='float32')\n",
    "for i, seqs in enumerate(padded_comment_output_sequences):\n",
    "    for j, seq in enumerate(seqs):\n",
    "        if j > 0:\n",
    "            decoder_targets_one_hot[i, j, seq] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the encoder\n",
    "\n",
    "latent_dim = 512 #LSTM_NODES = latent_dim. Either set to 256 or 50???\n",
    "\n",
    "encoder_inputs = Input(shape=(AVG_POST_LEN,)) # shape=(None,), dtype=\"int64\"\n",
    "enc_emb = embedding_layer(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the decoder\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(AVG_COMMENT_LEN,)) \n",
    "dec_emb_layer = Embedding(vocab_size+1, latent_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from the decoder LSTM\n",
    "\n",
    "decoder_dense = Dense(vocab_size+1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='../LSTM/model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_inputs.shape, decoder_inputs.shape)\n",
    "print(decoder_inputs.shape)\n",
    "print(padded_post_sequences.shape)\n",
    "print(padded_comment_input_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r = model.fit(\n",
    "    x=[padded_post_sequences, padded_comment_input_sequences],\n",
    "    y=decoder_targets_one_hot,\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "model.save(\"s2s\")\n",
    "#model.save_weights('saved_weights.hdf5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run overnight\n",
    "# Increase word embeddings to 200\n",
    "parameters = [\n",
    "    {'epochs': 200, 'latent_dim': 512, 'optimizer': 'adam', ''}, \n",
    "    {'epochs': 200, 'latent_dim': 512, 'optimizer': 'adam', ''}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 203)]        0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 95)]         0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 203, 100)     845400      input_1[0][0]                    \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 95, 512)      4328448     input_2[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     [(None, 512), (None, 1255424     embedding[0][0]                  \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   [(None, 95, 512), (N 2099200     embedding_1[0][0]                \n                                                                 lstm[0][1]                       \n                                                                 lstm[0][2]                       \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 95, 8454)     4336902     lstm_1[0][0]                     \n==================================================================================================\nTotal params: 12,865,374\nTrainable params: 12,865,374\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"latent_dim_512_s2s\")\n",
    "model.summary()\n",
    "# for idx, layer in enumerate(model.layers):\n",
    "#     print(idx, layer.name)\n",
    "# return\n",
    "latent_dim = 512\n",
    "\"\"\"\n",
    "encoder_inputs = Input(shape=(AVG_POST_LEN,)) # shape=(None,), dtype=\"int64\"\n",
    "enc_emb = embedding_layer(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(AVG_COMMENT_LEN,)) \n",
    "dec_emb_layer = Embedding(vocab_size+1, latent_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "# for i in range(6):\n",
    "#     print(i)\n",
    "#     print(model.layers[i].output)\n",
    "# embedding_layer = model.layers[2]\n",
    "# enc_emb = embedding_layer(encoder_inputs)\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[4].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "\n",
    "# for i in range(4):\n",
    "#     print(model.input[i])\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "dec_emb_layer = model.layers[3]\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = model.layers[5]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    dec_emb, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[6]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "from keras.utils import plot_model\n",
    "plot_model(encoder_model, to_file='../LSTM/encoder_inference_model.png', show_shapes=True, show_layer_names=True)\n",
    "plot_model(decoder_model, to_file='../LSTM/decoder_inference_model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "def give_encouragement(input_seq):\n",
    "    #Marathi one\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = word_to_index['<START>']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_word[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '<END>' or len(decoded_sentence) > AVG_COMMENT_LEN):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 95) for input Tensor(\"input_2_10:0\", shape=(None, 95), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
      "-\n",
      "Post:  just wanted to say that you are the best !\n",
      "Generated Comment:   there are people who care you can overcome <END>\n",
      "Actual Comment:  <START> how bout me ?\n",
      "-\n",
      "Post:  encourage me to not fuck up\n",
      "Generated Comment:   there are people who care you can overcome <END>\n",
      "Actual Comment:  <START> as an scper , i am personally rooting for you . even though some people give not so nice feedback on your writing , i highly admire your persistence in trying to write an scp . keep writing and i guarantee you will see improvement . seriously , i hope this helps .\n",
      "-\n",
      "Post:  been feeling like crap for about a year now , thought about creating a blog to vent it out since i cant seem to speak the words out loud . i have started and am now slipping back again . i cant share it with the people i know , yet at least . just struggling with going through the motions yet again .\n",
      "Generated Comment:   there are people who care about you . it does not matter what you are feeling discouraged and foolish\n",
      "Actual Comment:  <START> venting certainly helps relieve stress whether it is on a blog , in some artwork , written letters . . . however you can express yourself . the negative experiences tend to stick in our minds , and it is good to let them pass through . i would also encourage you to focus on positive things too . we tend to become the things we focus on . it helps to remember that there are blessings all around us , even in bad times . even one enjoyable moment , savored and recalled , can lead us to seek more .\n",
      "-\n",
      "Post:  thursday i got let go from my job . in the last three years since moving to portland i have lost every job ive had . the career ive been working in i am burnt out in . i know that if i get a job in this same career , its just going to happen again . i have a wife and two kids and i just kinda need some random strangers to tell me things will be okay long enough for me to actually believe it .\n",
      "Generated Comment:   there are people who care about you . i am a bot . but you have to find another job . ive found\n",
      "Actual Comment:  <START> it is gonna be ok man . i am sorry you are dealing with this burden . you will find your way . breathe\n",
      "-\n",
      "Post:  she is a friend of mine , we hung out the other day at a carnival and when we rode a ferris wheel the guy letting us on called us lovebirds so we treated the day as a mock date . she sent me a message saying that was the most fun date i have been on and we are not even dating . i want to send her a message back saying since we had such a good time on our fake date , would you wanna go on a real one and while i think i have a good shot , i am still a bit worried about it . m17 and f18\n",
      "Generated Comment:   there are people who care you are doing well . i am a bot . downvote to your psyche if you have\n",
      "Actual Comment:  <START> shes into you and not even subtle . so happy for you ! ask her out and dont look back !\n"
     ]
    }
   ],
   "source": [
    "# for _ in range(5):\n",
    "# for seq_index in range(2):\n",
    "#     # Take one sequence (part of the training set)\n",
    "#     # for trying out decoding.\n",
    "#     input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "for _ in range(5):\n",
    "    i = np.random.choice(len(posts))\n",
    "    input_seq = padded_post_sequences[i:i+1]\n",
    "    generated_comment = give_encouragement(input_seq)\n",
    "    print('-')\n",
    "    print('Post: ', posts[i])\n",
    "    print('Generated Comment: ', generated_comment)\n",
    "    print(\"Actual Comment: \", comments_input[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(encoder_inputs.shape, encoder_states[0].shape, encoder_states[1].shape)\n",
    "# print(decoder_state_input_h.shape)\n",
    "# print(decoder_state_input_c.shape)\n",
    "# print(decoder_inputs_single.shape)\n",
    "# print(decoder_outputs.shape)\n",
    "# for i in decoder_states_inputs:\n",
    "#     print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "# # Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "plot_model(encoder_model, to_file='../LSTM/model_plot_enc.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = dec_emb_layer(decoder_inputs_single)\n",
    "\n",
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# # To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "# decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "# decoder_states2 = [state_h2, state_c2]\n",
    "# decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# # Encode the input sequence to get the \"thought vectors\"\n",
    "# encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# # Decoder setup\n",
    "# # Below tensors will hold the states of the previous time step\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# # To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "# decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "# decoder_states2 = [state_h2, state_c2]\n",
    "# decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# # Final decoder model\n",
    "# decoder_model = Model(\n",
    "#     [decoder_inputs] + decoder_states_inputs,\n",
    "#     [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "\n",
    "plot_model(decoder_model, to_file='../LSTM/model_plot_dec.png', show_shapes=True, show_layer_names=True)\n",
    "encoder_model.save_weights('encoder_model_weights.hdf5', overwrite=True)\n",
    "decoder_model.save_weights('decoder_model_weights.hdf5', overwrite=True)"
   ]
  },
  {
   "source": [
    "Keras LSTM Architecture\n",
    "\n",
    "The input shape of the text data is ordered as follows: batch size, number of time steps, hidden size (size of the hidden layer)\n",
    "For each batch sample and each word in the number of time steps, there is a l500 length embedding word vector to "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_model.load_weights(\"../LSTM/encoder_model_weights.hdf5\")\n",
    "    # decoder_model.load_weights(\"../LSTM/decoder_model_weights.hdf5\")\n",
    "    \n",
    "    # # KERAS\n",
    "    # states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # # Generate empty target sequence of length 1.\n",
    "    # target_seq = np.zeros((1, 1, vocab_size))\n",
    "    # # Populate the first character of target sequence with the start character.\n",
    "    # target_seq[0, 0, word_to_index['<START>']] = 1.0\n",
    "\n",
    "    # # Sampling loop for a batch of sequences\n",
    "    # # (to simplify, here we assume a batch of size 1).\n",
    "    # stop_condition = False\n",
    "    # decoded_sentence = \"\"\n",
    "    # while not stop_condition:\n",
    "    #     output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "    #     # Sample a token\n",
    "    #     sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    #     sampled_char = index_to_word[sampled_token_index]\n",
    "    #     decoded_sentence += sampled_char\n",
    "\n",
    "    #     # Exit condition: either hit max length\n",
    "    #     # or find stop character.\n",
    "    #     if sampled_char == \"<END>\" or len(decoded_sentence) > AVG_COMMENT_LEN:\n",
    "    #         stop_condition = True\n",
    "\n",
    "    #     # Update the target sequence (of length 1).\n",
    "    #     target_seq = np.zeros((1, 1, word_to_index['<START>']))\n",
    "    #     target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "    #     # Update states\n",
    "    #     states_value = [h, c]\n",
    "    # return decoded_sentence\n",
    "\n",
    "    # # The french one\n",
    "    # print(input_seq.shape)\n",
    "    # states_value = encoder_model.predict(input_seq)\n",
    "    # target_seq = np.zeros((1, 1))\n",
    "    # target_seq[0, 0] = word_to_index['<START>']\n",
    "    # eos = word_to_index['<END>']\n",
    "    # output_sentence = []\n",
    "    # for _ in range(AVG_COMMENT_LEN):\n",
    "    #     # print(states_value)\n",
    "    #     # print(target_seq.shape)\n",
    "    #     # inputs = [target_seq] + states_value\n",
    "    #     # print(inputs)\n",
    "    #     # print(states_value)\n",
    "    #     # for i in range(len(states_value)):\n",
    "    #     #     states_value[i] = np.asarray(states_value[i])\n",
    "    #     # print(states_value)\n",
    "    #     # temp = [target_seq]\n",
    "    #     # print(temp)\n",
    "    #     # temp.append(states_value)\n",
    "    #     # for i in [target_seq] + states_value:\n",
    "    #         # print(i.shape)\n",
    "    #     output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    #     idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "    #     if eos == idx:\n",
    "    #         break\n",
    "\n",
    "    #     word = ''\n",
    "\n",
    "    #     if idx > 0:\n",
    "    #         word = index_to_word[idx]\n",
    "    #         output_sentence.append(word)\n",
    "\n",
    "    #     target_seq[0, 0] = idx\n",
    "    #     states_value = [h, c]\n",
    "\n",
    "    # return ' '.join(output_sentence)\n",
    "\n",
    "    # The weird one\n",
    "    # #Getting the output states to pass into the decoder\n",
    "    # states_value = encoder_model.predict(input_seq)\n",
    "    # #Generating empty target sequence of length 1\n",
    "    # target_seq = np.zeros((1, 1, vocab_size+1))\n",
    "    # #Setting the first token of target sequence with the start token\n",
    "    # target_seq[0, 0, word_to_index['<START>']] = 1.\n",
    "    \n",
    "    # #A variable to store our response word by word\n",
    "    # decoded_sentence = ''\n",
    "    \n",
    "    # stop_condition = False\n",
    "    # while not stop_condition:\n",
    "    #   #Predicting output tokens with probabilities and states\n",
    "    #   output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq] + states_value)\n",
    "    #   #Choosing the one with highest probability\n",
    "    #   sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    #   sampled_token = index_to_word[sampled_token_index]\n",
    "    #   decoded_sentence += \" \" + sampled_token#Stop if hit max length or found the stop token\n",
    "    #   if (sampled_token == '<END>' or len(decoded_sentence) > AVG_COMMENT_LEN):\n",
    "    #       stop_condition = True\n",
    "    #   #Update the target sequence\n",
    "    #   target_seq = np.zeros((1, 1, vocab_size+1))\n",
    "    #   target_seq[0, 0, sampled_token_index] = 1.\n",
    "    #   #Update states\n",
    "    #   states_value = [hidden_state, cell_state]\n",
    "    # return decoded_sentence"
   ]
  }
 ]
}