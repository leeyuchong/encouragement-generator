{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import csv"
   ]
  },
  {
   "source": [
    "<h1>Text preprocessing</h1>\n",
    "Code adapted from: https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.replace(\"i'm\", \"i am\")\n",
    "    # text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = text.replace(\"he's\", \"he is\")\n",
    "    text = text.replace(\"she's\", \"she is\")\n",
    "    text = text.replace(\"it's\", \"it is\")\n",
    "    text = text.replace(\"what's\", \"that is\")\n",
    "    text = text.replace(\"that's\", \"that is\")\n",
    "    text = text.replace(\"where's\", \"where is\")\n",
    "    text = text.replace(\"how's\", \"how is\")\n",
    "    text = text.replace(\"\\'ll\", \" will\")\n",
    "    text = text.replace(\"\\'re\", \" are\")\n",
    "    text = text.replace(\"\\'ve\", \" have\")\n",
    "    text = text.replace(\"\\'d\", \" would\")\n",
    "    text = text.replace(\"won't\", \"will not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    text = text.replace(\"n'\", \"ng\")\n",
    "    text = text.replace(\"'bout\", \"about\")\n",
    "    text = text.replace(\"'til\", \"until\")\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # if text != \":)\" or text != \":(\":\n",
    "    #     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "start_char = \"<START>\"\n",
    "end_char = \"<END>\"\n",
    "\n",
    "posts = []\n",
    "comments_output = []\n",
    "comments_input = []\n",
    "with open('../splitted_data.csv', 'r', newline='') as csv_file:\n",
    "    textReader = csv.reader(csv_file)\n",
    "    for row in textReader:\n",
    "        cleaned_text = clean_text(row[1])\n",
    "        posts.append(\" \".join(re.findall(r\"[\\w']+|[.,!?;]\", cleaned_text)))\n",
    "\n",
    "        cleaned_text = clean_text(row[0])\n",
    "        cleaned_text = re.findall(r\"[\\w']+|[.,!?;]\", cleaned_text)\n",
    "        cleaned_text = [ x.strip() for x in cleaned_text ]\n",
    "        cleaned_text = [start_char] + cleaned_text + [end_char]\n",
    "        comments_output.append(\" \".join(cleaned_text[1:]))\n",
    "        comments_input.append(\" \".join(cleaned_text[:-1]))\n",
    "print(\"Num samples posts: \", len(posts))\n",
    "print(\"Num samples comments output\", len(comments_output))\n",
    "print(\"Num samples comments input\", len(comments_input))\n",
    "print(posts[500])\n",
    "print(comments_output[500])\n",
    "print(comments_input[500])"
   ]
  },
  {
   "source": [
    "# Generate all the unique words in the data set\n",
    "all_words = set()\n",
    "all_text = posts + comments_output + comments_input\n",
    "for sentence in all_text:\n",
    "    for word in sentence.split():\n",
    "        all_words.add(word)\n",
    "print(\"Total number of unique words: \", len(all_words))\n",
    "\n",
    "for i, word in enumerate(all_words):\n",
    "    if i < 5:\n",
    "        print(word)\n",
    "    else: break"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from scipy import stats\n",
    "\n",
    "vocab_size = len(all_words)\n",
    "tokenizer = Tokenizer(num_words = vocab_size+1, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "word_to_index = tokenizer.word_index\n",
    "index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "print(\"Total unique words: \", len(word_to_index))\n",
    "\n",
    "posts_sequence = tokenizer.texts_to_sequences(posts)\n",
    "MAX_POST_LEN = max(len(seq) for seq in posts_sequence)\n",
    "average_post_length = np.average([len(seq) for seq in posts_sequence])\n",
    "median_post_length = np.median([len(seq) for seq in posts_sequence])\n",
    "print(\"Max post length: \", MAX_POST_LEN)\n",
    "print(\"Average post length: \", average_post_length)\n",
    "print(\"median_post_length: \", median_post_length)\n",
    "\n",
    "comments_output_sequence = tokenizer.texts_to_sequences(comments_output)\n",
    "comments_input_sequence = tokenizer.texts_to_sequences(comments_input)\n",
    "MAX_COMMENT_LEN = max(len(seq) for seq in comments_output_sequence)\n",
    "average_comment_length = np.average([len(seq) for seq in comments_output_sequence])\n",
    "median_comment_length = np.median([len(seq) for seq in comments_output_sequence])\n",
    "print(\"Max comment length: \", MAX_COMMENT_LEN)\n",
    "print(\"Average comment length: \", average_comment_length)\n",
    "print(np.average([len(seq) for seq in comments_input_sequence]))\n",
    "print(\"median_comment_length: \", median_comment_length)\n",
    "\n",
    "AVG_POST_LEN = int(round(average_post_length))\n",
    "print(AVG_POST_LEN)\n",
    "AVG_COMMENT_LEN = int(round(average_comment_length))\n",
    "print(AVG_COMMENT_LEN)\n",
    "\n",
    "# index_to_word = dict()\n",
    "# for k, v in word_to_index.items():\n",
    "#     index_to_word[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the output to the average length of a comment (203)\n",
    "truncated = []\n",
    "for comment in comments_input_sequence:\n",
    "    if len(comment) > 95:\n",
    "        truncated.append(comment[:95])\n",
    "    else:\n",
    "        truncated.append(comment)\n",
    "comments_input_sequence = truncated.copy()\n",
    "print(comments_input_sequence[:5])\n",
    "truncated = []\n",
    "\n",
    "for comment in comments_output_sequence:\n",
    "    if len(comment) > 95:\n",
    "        truncated.append(comment[:94] + [word_to_index['<END>']])\n",
    "    else:\n",
    "        truncated.append(comment)\n",
    "comments_output_sequence = truncated.copy()\n",
    "print(comments_output_sequence[:5])\n",
    "\n",
    "print(np.average([len(seq) for seq in comments_output_sequence]))\n",
    "print(np.average([len(seq) for seq in comments_input_sequence]))\n",
    "print(max([len(seq) for seq in comments_output_sequence]))\n",
    "print(max([len(seq) for seq in comments_input_sequence]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PADDING\n",
    "\n",
    "padded_post_sequences = pad_sequences(posts_sequence, maxlen=AVG_POST_LEN, truncating='post')\n",
    "padded_comment_input_sequences = pad_sequences(comments_input_sequence, maxlen=AVG_COMMENT_LEN, padding='post')\n",
    "padded_comment_output_sequences = pad_sequences(comments_output_sequence, maxlen=AVG_COMMENT_LEN, padding='post')\n",
    "print(\"padded_post_sequences.shape\", padded_post_sequences.shape)\n",
    "print(\"padded_post_sequences[500]\", padded_post_sequences[500])\n",
    "print(word_to_index['we'])\n",
    "print(padded_comment_input_sequences.shape)\n",
    "print(padded_comment_input_sequences[500])\n",
    "print(word_to_index['<START>'])\n",
    "\n",
    "# encoder_sequences = tokenizer.texts_to_sequences(posts)\n",
    "# encoder_sequences_padded = pad_sequences(encoder_sequences, maxlen=max_source_length, dtype='int32', padding='post', truncating='post')\n",
    "# decoder_sequences = tokenizer.texts_to_sequences(comments)\n",
    "# decoder_sequences_padded = pad_sequences(decoder_sequences, maxlen=max_target_length, dtype='int32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer will consider the top 20000 words and will pad or truncate words to be 200 words long \n",
    "# vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "# text_ds = tf.data.Dataset.from_tensor_slices(comments).batch(128)\n",
    "# vectorizer.adapt(text_ds)\n",
    "# print(vectorizer.get_vocabulary()[:5])\n",
    "\n",
    "#dict mapping words to their indices\n",
    "# voc = vectorizer.get_vocabulary()\n",
    "# word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "embeddings_dictionary = {}\n",
    "with open('./glove6B/glove.6B.200d.txt', 'r') as glove:\n",
    "    for line in glove:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
    "for word, i in word_to_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "print(embeddings_dictionary[\"we\"])\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "source": [
    "<h1>Embedding Layer</h1>\n",
    "The size of the embedding layer is the size of the vector that represents each word. We usually match the size of the embedding layer output with the number of hidden layers in the LSTM cell. \n",
    "\n",
    "The size of the hidden layer is equal to the number nodes representing the signmoid, tanh and hidden state layer in the LSTM cell. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "# embedding_layer = Embedding(vocab_size+1, embedding_dim, embeddings_initializer=Constant(embedding_matrix),  trainable=False)\n",
    "print(vocab_size+1)\n",
    "embedding_layer = Embedding(vocab_size+1, embedding_dim, embeddings_initializer=Constant(embedding_matrix), input_length=AVG_POST_LEN, trainable=False)\n"
   ]
  },
  {
   "source": [
    "CREATING THE MODEL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets_one_hot = np.zeros((\n",
    "    len(posts),\n",
    "    AVG_COMMENT_LEN,\n",
    "    vocab_size+1\n",
    "    ), \n",
    "    dtype='float32' \n",
    ")\n",
    "print(decoder_targets_one_hot.shape)\n",
    "\n",
    "# # One-hot encoding of the output\n",
    "# num_samples = len(encoder_sequences)\n",
    "# decoder_output_data = np.zeros((num_samples, max_target_length, vocab_size), dtype='float32')\n",
    "for i, seqs in enumerate(padded_comment_output_sequences):\n",
    "    for j, seq in enumerate(seqs):\n",
    "        if j > 0:\n",
    "            decoder_targets_one_hot[i, j, seq] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the encoder\n",
    "\n",
    "latent_dim = 256 #LSTM_NODES = latent_dim. Either set to 256 or 50???\n",
    "\n",
    "encoder_inputs = Input(shape=(AVG_POST_LEN,)) # shape=(None,), dtype=\"int64\"\n",
    "enc_emb = embedding_layer(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the decoder\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(AVG_COMMENT_LEN,)) \n",
    "dec_emb_layer = Embedding(vocab_size+1, latent_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from the decoder LSTM\n",
    "\n",
    "decoder_dense = Dense(vocab_size+1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import SGD\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='../LSTM/model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_inputs.shape, decoder_inputs.shape)\n",
    "print(decoder_inputs.shape)\n",
    "print(padded_post_sequences.shape)\n",
    "print(padded_comment_input_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "r = model.fit(\n",
    "    x=[padded_post_sequences, padded_comment_input_sequences],\n",
    "    y=decoder_targets_one_hot,\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "model.save(\"s2s\")\n",
    "#model.save_weights('saved_weights.hdf5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run overnight\n",
    "# Increase word embeddings to 200\n",
    "parameters = [\n",
    "    {'epochs': 200, 'latent_dim': 256, 'optimizer': 'adam', ''}, \n",
    "    {'epochs': 200, 'latent_dim': 256, 'optimizer': 'adam', ''}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"latent_dim_512_s2s\")\n",
    "model.summary()\n",
    "# for idx, layer in enumerate(model.layers):\n",
    "#     print(idx, layer.name)\n",
    "# return\n",
    "latent_dim = 256\n",
    "\"\"\"\n",
    "encoder_inputs = Input(shape=(AVG_POST_LEN,)) # shape=(None,), dtype=\"int64\"\n",
    "enc_emb = embedding_layer(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(AVG_COMMENT_LEN,)) \n",
    "dec_emb_layer = Embedding(vocab_size+1, latent_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "# for i in range(6):\n",
    "#     print(i)\n",
    "#     print(model.layers[i].output)\n",
    "# embedding_layer = model.layers[2]\n",
    "# enc_emb = embedding_layer(encoder_inputs)\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[4].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "\n",
    "# for i in range(4):\n",
    "#     print(model.input[i])\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "dec_emb_layer = model.layers[3]\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = model.layers[5]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    dec_emb, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[6]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "from keras.utils import plot_model\n",
    "plot_model(encoder_model, to_file='../LSTM/encoder_inference_model.png', show_shapes=True, show_layer_names=True)\n",
    "plot_model(decoder_model, to_file='../LSTM/decoder_inference_model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "def give_encouragement(input_seq):\n",
    "    #Marathi one\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = word_to_index['<START>']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_word[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '<END>' or len(decoded_sentence) > AVG_COMMENT_LEN):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for _ in range(5):\n",
    "# for seq_index in range(2):\n",
    "#     # Take one sequence (part of the training set)\n",
    "#     # for trying out decoding.\n",
    "#     input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "for _ in range(5):\n",
    "    i = np.random.choice(len(posts))\n",
    "    input_seq = padded_post_sequences[i:i+1]\n",
    "    generated_comment = give_encouragement(input_seq)\n",
    "    print('-')\n",
    "    print('Post: ', posts[i])\n",
    "    print('Generated Comment: ', generated_comment)\n",
    "    print(\"Actual Comment: \", comments_input[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(encoder_inputs.shape, encoder_states[0].shape, encoder_states[1].shape)\n",
    "# print(decoder_state_input_h.shape)\n",
    "# print(decoder_state_input_c.shape)\n",
    "# print(decoder_inputs_single.shape)\n",
    "# print(decoder_outputs.shape)\n",
    "# for i in decoder_states_inputs:\n",
    "#     print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "# # Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "plot_model(encoder_model, to_file='../LSTM/model_plot_enc.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = dec_emb_layer(decoder_inputs_single)\n",
    "\n",
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# # To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "# decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "# decoder_states2 = [state_h2, state_c2]\n",
    "# decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# # Encode the input sequence to get the \"thought vectors\"\n",
    "# encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# # Decoder setup\n",
    "# # Below tensors will hold the states of the previous time step\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# # To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "# decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "# decoder_states2 = [state_h2, state_c2]\n",
    "# decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# # Final decoder model\n",
    "# decoder_model = Model(\n",
    "#     [decoder_inputs] + decoder_states_inputs,\n",
    "#     [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "\n",
    "plot_model(decoder_model, to_file='../LSTM/model_plot_dec.png', show_shapes=True, show_layer_names=True)\n",
    "encoder_model.save_weights('encoder_model_weights.hdf5', overwrite=True)\n",
    "decoder_model.save_weights('decoder_model_weights.hdf5', overwrite=True)"
   ]
  },
  {
   "source": [
    "Keras LSTM Architecture\n",
    "\n",
    "The input shape of the text data is ordered as follows: batch size, number of time steps, hidden size (size of the hidden layer)\n",
    "For each batch sample and each word in the number of time steps, there is a l500 length embedding word vector to "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_model.load_weights(\"../LSTM/encoder_model_weights.hdf5\")\n",
    "    # decoder_model.load_weights(\"../LSTM/decoder_model_weights.hdf5\")\n",
    "    \n",
    "    # # KERAS\n",
    "    # states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # # Generate empty target sequence of length 1.\n",
    "    # target_seq = np.zeros((1, 1, vocab_size))\n",
    "    # # Populate the first character of target sequence with the start character.\n",
    "    # target_seq[0, 0, word_to_index['<START>']] = 1.0\n",
    "\n",
    "    # # Sampling loop for a batch of sequences\n",
    "    # # (to simplify, here we assume a batch of size 1).\n",
    "    # stop_condition = False\n",
    "    # decoded_sentence = \"\"\n",
    "    # while not stop_condition:\n",
    "    #     output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "    #     # Sample a token\n",
    "    #     sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    #     sampled_char = index_to_word[sampled_token_index]\n",
    "    #     decoded_sentence += sampled_char\n",
    "\n",
    "    #     # Exit condition: either hit max length\n",
    "    #     # or find stop character.\n",
    "    #     if sampled_char == \"<END>\" or len(decoded_sentence) > AVG_COMMENT_LEN:\n",
    "    #         stop_condition = True\n",
    "\n",
    "    #     # Update the target sequence (of length 1).\n",
    "    #     target_seq = np.zeros((1, 1, word_to_index['<START>']))\n",
    "    #     target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "    #     # Update states\n",
    "    #     states_value = [h, c]\n",
    "    # return decoded_sentence\n",
    "\n",
    "    # # The french one\n",
    "    # print(input_seq.shape)\n",
    "    # states_value = encoder_model.predict(input_seq)\n",
    "    # target_seq = np.zeros((1, 1))\n",
    "    # target_seq[0, 0] = word_to_index['<START>']\n",
    "    # eos = word_to_index['<END>']\n",
    "    # output_sentence = []\n",
    "    # for _ in range(AVG_COMMENT_LEN):\n",
    "    #     # print(states_value)\n",
    "    #     # print(target_seq.shape)\n",
    "    #     # inputs = [target_seq] + states_value\n",
    "    #     # print(inputs)\n",
    "    #     # print(states_value)\n",
    "    #     # for i in range(len(states_value)):\n",
    "    #     #     states_value[i] = np.asarray(states_value[i])\n",
    "    #     # print(states_value)\n",
    "    #     # temp = [target_seq]\n",
    "    #     # print(temp)\n",
    "    #     # temp.append(states_value)\n",
    "    #     # for i in [target_seq] + states_value:\n",
    "    #         # print(i.shape)\n",
    "    #     output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    #     idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "    #     if eos == idx:\n",
    "    #         break\n",
    "\n",
    "    #     word = ''\n",
    "\n",
    "    #     if idx > 0:\n",
    "    #         word = index_to_word[idx]\n",
    "    #         output_sentence.append(word)\n",
    "\n",
    "    #     target_seq[0, 0] = idx\n",
    "    #     states_value = [h, c]\n",
    "\n",
    "    # return ' '.join(output_sentence)\n",
    "\n",
    "    # The weird one\n",
    "    # #Getting the output states to pass into the decoder\n",
    "    # states_value = encoder_model.predict(input_seq)\n",
    "    # #Generating empty target sequence of length 1\n",
    "    # target_seq = np.zeros((1, 1, vocab_size+1))\n",
    "    # #Setting the first token of target sequence with the start token\n",
    "    # target_seq[0, 0, word_to_index['<START>']] = 1.\n",
    "    \n",
    "    # #A variable to store our response word by word\n",
    "    # decoded_sentence = ''\n",
    "    \n",
    "    # stop_condition = False\n",
    "    # while not stop_condition:\n",
    "    #   #Predicting output tokens with probabilities and states\n",
    "    #   output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq] + states_value)\n",
    "    #   #Choosing the one with highest probability\n",
    "    #   sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    #   sampled_token = index_to_word[sampled_token_index]\n",
    "    #   decoded_sentence += \" \" + sampled_token#Stop if hit max length or found the stop token\n",
    "    #   if (sampled_token == '<END>' or len(decoded_sentence) > AVG_COMMENT_LEN):\n",
    "    #       stop_condition = True\n",
    "    #   #Update the target sequence\n",
    "    #   target_seq = np.zeros((1, 1, vocab_size+1))\n",
    "    #   target_seq[0, 0, sampled_token_index] = 1.\n",
    "    #   #Update states\n",
    "    #   states_value = [hidden_state, cell_state]\n",
    "    # return decoded_sentence"
   ]
  }
 ]
}