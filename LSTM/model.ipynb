{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import csv"
   ]
  },
  {
   "source": [
    "<h1>Text preprocessing</h1>\n",
    "Code adapted from: https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num samples posts:  2532\nNum samples comments output 2532\nNum samples comments input 2532\nplease , i just need something . please .\nabsolutely you will be okay . just remember that everything that has happened thus far in life has been okay , so it stands to reason that this will be okay too . relax . everything is in control . there's always a way . <END>\n<START> absolutely you will be okay . just remember that everything that has happened thus far in life has been okay , so it stands to reason that this will be okay too . relax . everything is in control . there's always a way .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.replace(\"i'm\", \"i am\")\n",
    "    # text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = text.replace(\"he's\", \"he is\")\n",
    "    text = text.replace(\"she's\", \"she is\")\n",
    "    text = text.replace(\"it's\", \"it is\")\n",
    "    text = text.replace(\"what's\", \"that is\")\n",
    "    text = text.replace(\"that's\", \"that is\")\n",
    "    text = text.replace(\"where's\", \"where is\")\n",
    "    text = text.replace(\"how's\", \"how is\")\n",
    "    text = text.replace(\"\\'ll\", \" will\")\n",
    "    text = text.replace(\"\\'re\", \" are\")\n",
    "    text = text.replace(\"\\'ve\", \" have\")\n",
    "    text = text.replace(\"\\'d\", \" would\")\n",
    "    text = text.replace(\"won't\", \"will not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    text = text.replace(\"n'\", \"ng\")\n",
    "    text = text.replace(\"'bout\", \"about\")\n",
    "    text = text.replace(\"'til\", \"until\")\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # if text != \":)\" or text != \":(\":\n",
    "    #     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "start_char = \"<START>\"\n",
    "end_char = \"<END>\"\n",
    "\n",
    "posts = []\n",
    "comments_output = []\n",
    "comments_input = []\n",
    "with open('../splitted_data_2.csv', 'r', newline='') as csv_file:\n",
    "    textReader = csv.reader(csv_file)\n",
    "    for row in textReader:\n",
    "        cleaned_text = clean_text(row[1])\n",
    "        posts.append(\" \".join(re.findall(r\"[\\w']+|[.,!?;]\", cleaned_text)))\n",
    "\n",
    "        cleaned_text = clean_text(row[0])\n",
    "        cleaned_text = re.findall(r\"[\\w']+|[.,!?;]\", cleaned_text)\n",
    "        cleaned_text = [ x.strip() for x in cleaned_text ]\n",
    "        cleaned_text = [start_char] + cleaned_text + [end_char]\n",
    "        comments_output.append(\" \".join(cleaned_text[1:]))\n",
    "        comments_input.append(\" \".join(cleaned_text[:-1]))\n",
    "print(\"Num samples posts: \", len(posts))\n",
    "print(\"Num samples comments output\", len(comments_output))\n",
    "print(\"Num samples comments input\", len(comments_input))\n",
    "print(posts[500])\n",
    "print(comments_output[500])\n",
    "print(comments_input[500])"
   ]
  },
  {
   "source": [
    "# Generate all the unique words in the data set\n",
    "all_words = set()\n",
    "all_text = posts + comments_output + comments_input\n",
    "for sentence in all_text:\n",
    "    for word in sentence.split():\n",
    "        all_words.add(word)\n",
    "print(\"Total number of unique words: \", len(all_words))\n",
    "\n",
    "for i, word in enumerate(all_words):\n",
    "    if i < 5:\n",
    "        print(word)\n",
    "    else: break"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of unique words:  8316\nhomeschooled\ncourse\nrelatives\ncouselor\npersons\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total unique words:  8316\n",
      "Max post length:  373\n",
      "Average post length:  121.51382306477093\n",
      "median_post_length:  129.0\n",
      "Max comment length:  209\n",
      "Average comment length:  69.35031595576619\n",
      "69.35031595576619\n",
      "median_comment_length:  68.0\n",
      "122\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from scipy import stats\n",
    "\n",
    "vocab_size = len(all_words)\n",
    "tokenizer = Tokenizer(num_words = vocab_size+1, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "word_to_index = tokenizer.word_index\n",
    "index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "print(\"Total unique words: \", len(word_to_index))\n",
    "\n",
    "posts_sequence = tokenizer.texts_to_sequences(posts)\n",
    "MAX_POST_LEN = max(len(seq) for seq in posts_sequence)\n",
    "average_post_length = np.average([len(seq) for seq in posts_sequence])\n",
    "median_post_length = np.median([len(seq) for seq in posts_sequence])\n",
    "print(\"Max post length: \", MAX_POST_LEN)\n",
    "print(\"Average post length: \", average_post_length)\n",
    "print(\"median_post_length: \", median_post_length)\n",
    "\n",
    "comments_output_sequence = tokenizer.texts_to_sequences(comments_output)\n",
    "comments_input_sequence = tokenizer.texts_to_sequences(comments_input)\n",
    "MAX_COMMENT_LEN = max(len(seq) for seq in comments_output_sequence)\n",
    "average_comment_length = np.average([len(seq) for seq in comments_output_sequence])\n",
    "median_comment_length = np.median([len(seq) for seq in comments_output_sequence])\n",
    "print(\"Max comment length: \", MAX_COMMENT_LEN)\n",
    "print(\"Average comment length: \", average_comment_length)\n",
    "print(np.average([len(seq) for seq in comments_input_sequence]))\n",
    "print(\"median_comment_length: \", median_comment_length)\n",
    "\n",
    "AVG_POST_LEN = int(round(average_post_length))\n",
    "print(AVG_POST_LEN)\n",
    "AVG_COMMENT_LEN = int(round(average_comment_length))\n",
    "print(AVG_COMMENT_LEN)\n",
    "\n",
    "# index_to_word = dict()\n",
    "# for k, v in word_to_index.items():\n",
    "#     index_to_word[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[39, 20, 221, 11, 422, 619, 15, 80, 187, 115, 5, 66, 10, 6, 25, 188, 1281, 14, 37, 1255, 2126, 157, 1, 6, 17, 3, 50, 8, 991, 3, 91, 209, 8, 5104, 12, 5105, 4, 18, 42, 8, 202, 47, 4, 291, 5106, 3, 22, 7, 311, 967, 14, 23, 511, 5, 5107, 72, 1], [39, 613, 30, 51, 3, 1322, 8, 1991, 1, 2638, 76, 1474, 4, 2639, 4, 13, 440, 1410, 7, 2640, 594, 10, 1168, 944, 24, 285, 8, 1644, 3, 2147, 944, 11, 262, 2641, 4, 74, 27, 2530, 1674, 1, 23, 528, 31, 87, 9, 11, 2407, 5, 31, 2642, 172, 2270, 1, 1443, 3, 98, 3, 547, 315, 944, 29, 7, 1168, 1003, 127, 102, 66, 37, 180, 1267, 88, 1, 811, 29, 23, 661, 30, 55, 77, 3, 288, 2643, 15, 8, 235, 1], [39, 107, 4, 2, 188, 752, 3, 136, 217, 724, 5, 99, 333, 70, 133, 12, 7, 1049, 6, 87, 6, 24, 4, 6, 24, 7, 797, 3, 20, 198, 1, 67, 11, 7, 140, 12, 51, 10, 6, 17, 1095, 5, 31, 585, 3, 20, 198, 1, 81, 53, 6, 25, 82, 3, 97, 7, 182, 47, 3, 97, 125, 12, 72, 1, 172, 2550, 5, 117, 6, 17, 11, 775, 14, 23, 661, 15, 20, 198, 1], [39, 4236, 355, 330, 3512, 60, 58, 33, 16, 4237, 15, 8, 2061, 10, 102, 2735, 413, 12, 2291, 34, 705, 1, 28, 4, 13, 117, 54, 6, 352, 3, 8, 2444, 5, 612, 1304, 4, 89, 150, 6, 4238, 8, 4239, 128, 1], [39, 2, 104, 20, 47, 12, 15, 754, 2405, 4, 377, 5, 812, 31, 863, 6, 3, 4240, 96, 6, 24, 4, 48, 6, 368, 5, 48, 6, 62, 1]]\n[[20, 221, 11, 422, 619, 15, 80, 187, 115, 5, 66, 10, 6, 25, 188, 1281, 14, 37, 1255, 2126, 157, 1, 6, 17, 3, 50, 8, 991, 3, 91, 209, 8, 5104, 12, 5105, 4, 18, 42, 8, 202, 47, 4, 291, 5106, 3, 22, 7, 311, 967, 14, 23, 511, 5, 5107, 72, 1, 38], [613, 30, 51, 3, 1322, 8, 1991, 1, 2638, 76, 1474, 4, 2639, 4, 13, 440, 1410, 7, 2640, 594, 10, 1168, 944, 24, 285, 8, 1644, 3, 2147, 944, 11, 262, 2641, 4, 74, 27, 2530, 1674, 1, 23, 528, 31, 87, 9, 11, 2407, 5, 31, 2642, 172, 2270, 1, 1443, 3, 98, 3, 547, 315, 944, 29, 7, 1168, 1003, 127, 102, 66, 37, 180, 1267, 88, 1, 811, 29, 23, 661, 30, 55, 77, 3, 288, 2643, 15, 8, 235, 1, 38], [107, 4, 2, 188, 752, 3, 136, 217, 724, 5, 99, 333, 70, 133, 12, 7, 1049, 6, 87, 6, 24, 4, 6, 24, 7, 797, 3, 20, 198, 1, 67, 11, 7, 140, 12, 51, 10, 6, 17, 1095, 5, 31, 585, 3, 20, 198, 1, 81, 53, 6, 25, 82, 3, 97, 7, 182, 47, 3, 97, 125, 12, 72, 1, 172, 2550, 5, 117, 6, 17, 11, 775, 14, 23, 661, 15, 20, 198, 1, 38], [4236, 355, 330, 3512, 60, 58, 33, 16, 4237, 15, 8, 2061, 10, 102, 2735, 413, 12, 2291, 34, 705, 1, 28, 4, 13, 117, 54, 6, 352, 3, 8, 2444, 5, 612, 1304, 4, 89, 150, 6, 4238, 8, 4239, 128, 1, 38], [2, 104, 20, 47, 12, 15, 754, 2405, 4, 377, 5, 812, 31, 863, 6, 3, 4240, 96, 6, 24, 4, 48, 6, 368, 5, 48, 6, 62, 1, 38]]\n61.95932069510268\n61.95932069510268\n95\n95\n"
     ]
    }
   ],
   "source": [
    "# Truncate the output to the average length of a comment (203)\n",
    "truncated = []\n",
    "for comment in comments_input_sequence:\n",
    "    if len(comment) > 95:\n",
    "        truncated.append(comment[:95])\n",
    "    else:\n",
    "        truncated.append(comment)\n",
    "comments_input_sequence = truncated.copy()\n",
    "print(comments_input_sequence[:5])\n",
    "truncated = []\n",
    "\n",
    "for comment in comments_output_sequence:\n",
    "    if len(comment) > 95:\n",
    "        truncated.append(comment[:94] + [word_to_index['<END>']])\n",
    "    else:\n",
    "        truncated.append(comment)\n",
    "comments_output_sequence = truncated.copy()\n",
    "print(comments_output_sequence[:5])\n",
    "\n",
    "print(np.average([len(seq) for seq in comments_output_sequence]))\n",
    "print(np.average([len(seq) for seq in comments_input_sequence]))\n",
    "print(max([len(seq) for seq in comments_output_sequence]))\n",
    "print(max([len(seq) for seq in comments_input_sequence]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "padded_post_sequences.shape (2532, 122)\npadded_post_sequences[500] [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0 164   4   2  25  82  83   1 164   1]\n65\n(2532, 69)\n[  39  436    6   31   22  260    1   25  263   10  143   10  108  374\n 2097  281   15   59  108   63  260    4   28    9 3826    3  416   10\n   20   31   22  260  111    1 1098    1  143   11   15  451    1  366\n  141    7  115    1    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0]\n39\n"
     ]
    }
   ],
   "source": [
    "#PADDING\n",
    "\n",
    "padded_post_sequences = pad_sequences(posts_sequence, maxlen=AVG_POST_LEN, truncating='post')\n",
    "padded_comment_input_sequences = pad_sequences(comments_input_sequence, maxlen=AVG_COMMENT_LEN, padding='post')\n",
    "padded_comment_output_sequences = pad_sequences(comments_output_sequence, maxlen=AVG_COMMENT_LEN, padding='post')\n",
    "print(\"padded_post_sequences.shape\", padded_post_sequences.shape)\n",
    "print(\"padded_post_sequences[500]\", padded_post_sequences[500])\n",
    "print(word_to_index['we'])\n",
    "print(padded_comment_input_sequences.shape)\n",
    "print(padded_comment_input_sequences[500])\n",
    "print(word_to_index['<START>'])\n",
    "\n",
    "# encoder_sequences = tokenizer.texts_to_sequences(posts)\n",
    "# encoder_sequences_padded = pad_sequences(encoder_sequences, maxlen=max_source_length, dtype='int32', padding='post', truncating='post')\n",
    "# decoder_sequences = tokenizer.texts_to_sequences(comments)\n",
    "# decoder_sequences_padded = pad_sequences(decoder_sequences, maxlen=max_target_length, dtype='int32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Layer will consider the top 20000 words and will pad or truncate words to be 200 words long \n",
    "# vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "# text_ds = tf.data.Dataset.from_tensor_slices(comments).batch(128)\n",
    "# vectorizer.adapt(text_ds)\n",
    "# print(vectorizer.get_vocabulary()[:5])\n",
    "\n",
    "#dict mapping words to their indices\n",
    "# voc = vectorizer.get_vocabulary()\n",
    "# word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "embeddings_dictionary = {}\n",
    "with open('./glove6B/glove.6B.200d.txt', 'r') as glove:\n",
    "    for line in glove:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converted 7786 words (530 misses)\n[ 0.19495    0.6061    -0.077356   0.017301  -0.5137     0.22422\n -0.80773    0.022378   0.30256    1.0667    -0.10918    0.57903\n  0.23986    0.1691     0.0072384  0.42198   -0.20459    0.60271\n  0.19188   -0.19616    0.3307     3.2002    -0.18104    0.20784\n  0.49511   -0.42259    0.022125   0.24379    0.16714   -0.2091\n -0.12489   -0.51767   -0.13569   -0.2598    -0.17961   -0.47663\n -0.8954    -0.27139    0.17746    0.45827    0.21363    0.22343\n -0.049342   0.34286   -0.32315    0.2747     0.95993   -0.25979\n  0.2125    -0.21373    0.19809    0.15455   -0.48581    0.38925\n  0.33747   -0.27898    0.19371   -0.45872   -0.054217  -0.24023\n  0.59153    0.12453   -0.21302    0.058223  -0.046671  -0.011614\n -0.32026    0.6412    -0.28719    0.035138   0.39287   -0.030683\n  0.083529  -0.010964  -0.62427   -0.13575   -0.38469    0.11454\n -0.61038    0.12595   -0.17633   -0.21415   -0.37014    0.21763\n  0.055366  -0.25242   -0.45476   -0.28105    0.18911   -1.5854\n  0.64841   -0.34622    0.59254   -0.39034   -0.44258    0.20562\n  0.44395    0.2302    -0.35018   -0.3609     0.62993   -0.34699\n -0.31965   -0.17361    0.51715    0.68493   -0.15587    1.4255\n -0.94314    0.031951  -0.2621    -0.089544   0.22437   -0.050374\n  0.035414  -0.070493   0.17037   -0.38598    0.0095626  0.26363\n  0.72125   -0.13797    0.70602   -0.50839   -0.49722   -0.48706\n  0.16254    0.025619   0.33572   -0.6416    -0.32542    0.21896\n  0.05331    0.082011   0.12038    0.088849  -0.04651   -0.085435\n  0.036835  -0.14695   -0.25841   -0.043812   0.053209  -0.48955\n  1.7395     0.99015    0.09395   -0.20236   -0.050135   0.18338\n  0.22714    0.83146   -0.30974    0.51996    0.068264  -0.28237\n -0.30097   -0.031014   0.024615   0.4294    -0.085036   0.051913\n  0.31251   -0.34443   -0.085145   0.024975   0.0082017  0.17241\n -0.66001    0.0058962 -0.055387  -0.22315    0.35722   -0.18962\n  0.2582    -0.24685   -0.79572   -0.09436   -0.10271    0.13713\n  1.4866    -0.57114   -0.5265    -0.25181   -0.40792   -0.18612\n  0.040009   0.11557    0.017987   0.27149   -0.14252   -0.087756\n  0.15196    0.064926   0.01651   -0.25334    0.27437    0.24246\n  0.018494   0.22473  ]\n(8317, 200)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 200\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
    "for word, i in word_to_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "print(embeddings_dictionary[\"we\"])\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "source": [
    "<h1>Embedding Layer</h1>\n",
    "The size of the embedding layer is the size of the vector that represents each word. We usually match the size of the embedding layer output with the number of hidden layers in the LSTM cell. \n",
    "\n",
    "The size of the hidden layer is equal to the number nodes representing the signmoid, tanh and hidden state layer in the LSTM cell. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8317\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import Constant\n",
    "# embedding_layer = Embedding(vocab_size+1, embedding_dim, embeddings_initializer=Constant(embedding_matrix),  trainable=False)\n",
    "print(vocab_size+1)\n",
    "embedding_layer = Embedding(vocab_size+1, embedding_dim, embeddings_initializer=Constant(embedding_matrix), input_length=AVG_POST_LEN, trainable=False)\n"
   ]
  },
  {
   "source": [
    "CREATING THE MODEL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2532, 69, 8317)\n"
     ]
    }
   ],
   "source": [
    "decoder_targets_one_hot = np.zeros((\n",
    "    len(posts),\n",
    "    AVG_COMMENT_LEN,\n",
    "    vocab_size+1\n",
    "    ), \n",
    "    dtype='float32' \n",
    ")\n",
    "print(decoder_targets_one_hot.shape)\n",
    "\n",
    "# # One-hot encoding of the output\n",
    "# num_samples = len(encoder_sequences)\n",
    "# decoder_output_data = np.zeros((num_samples, max_target_length, vocab_size), dtype='float32')\n",
    "for i, seqs in enumerate(padded_comment_output_sequences):\n",
    "    for j, seq in enumerate(seqs):\n",
    "        if j > 0:\n",
    "            decoder_targets_one_hot[i, j, seq] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the encoder\n",
    "\n",
    "latent_dim = 256 #LSTM_NODES = latent_dim. Either set to 256 or 50???\n",
    "\n",
    "encoder_inputs = Input(shape=(AVG_POST_LEN,)) # shape=(None,), dtype=\"int64\"\n",
    "enc_emb = embedding_layer(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the decoder\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(AVG_COMMENT_LEN,)) \n",
    "dec_emb_layer = Embedding(vocab_size+1, latent_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from the decoder LSTM\n",
    "\n",
    "decoder_dense = Dense(vocab_size+1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import SGD\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='../LSTM/model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(None, 122) (None, 69)\n(None, 69)\n(2532, 122)\n(2532, 69)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_inputs.shape, decoder_inputs.shape)\n",
    "print(decoder_inputs.shape)\n",
    "print(padded_post_sequences.shape)\n",
    "print(padded_comment_input_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ": 0.4837 - val_loss: 3.0194 - val_accuracy: 0.4516\n",
      "Epoch 67/200\n",
      "36/36 [==============================] - 43s 1s/step - loss: 2.5432 - accuracy: 0.4857 - val_loss: 3.0129 - val_accuracy: 0.4552\n",
      "Epoch 68/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.5309 - accuracy: 0.4876 - val_loss: 3.0048 - val_accuracy: 0.4551\n",
      "Epoch 69/200\n",
      "36/36 [==============================] - 49s 1s/step - loss: 2.5182 - accuracy: 0.4896 - val_loss: 2.9967 - val_accuracy: 0.4575\n",
      "Epoch 70/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.5055 - accuracy: 0.4916 - val_loss: 2.9884 - val_accuracy: 0.4603\n",
      "Epoch 71/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.4927 - accuracy: 0.4935 - val_loss: 2.9820 - val_accuracy: 0.4606\n",
      "Epoch 72/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.4804 - accuracy: 0.4960 - val_loss: 2.9780 - val_accuracy: 0.4590\n",
      "Epoch 73/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.4679 - accuracy: 0.4975 - val_loss: 2.9685 - val_accuracy: 0.4633\n",
      "Epoch 74/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.4566 - accuracy: 0.4992 - val_loss: 2.9613 - val_accuracy: 0.4642\n",
      "Epoch 75/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.4445 - accuracy: 0.5013 - val_loss: 2.9542 - val_accuracy: 0.4667\n",
      "Epoch 76/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.4320 - accuracy: 0.5036 - val_loss: 2.9485 - val_accuracy: 0.4670\n",
      "Epoch 77/200\n",
      "36/36 [==============================] - 50s 1s/step - loss: 2.4205 - accuracy: 0.5050 - val_loss: 2.9416 - val_accuracy: 0.4680\n",
      "Epoch 78/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.4104 - accuracy: 0.5062 - val_loss: 2.9348 - val_accuracy: 0.4699\n",
      "Epoch 79/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.3989 - accuracy: 0.5089 - val_loss: 2.9302 - val_accuracy: 0.4692\n",
      "Epoch 80/200\n",
      "36/36 [==============================] - 43s 1s/step - loss: 2.3873 - accuracy: 0.5100 - val_loss: 2.9232 - val_accuracy: 0.4712\n",
      "Epoch 81/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.3765 - accuracy: 0.5118 - val_loss: 2.9164 - val_accuracy: 0.4716\n",
      "Epoch 82/200\n",
      "36/36 [==============================] - 43s 1s/step - loss: 2.3663 - accuracy: 0.5134 - val_loss: 2.9107 - val_accuracy: 0.4727\n",
      "Epoch 83/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.3550 - accuracy: 0.5153 - val_loss: 2.9038 - val_accuracy: 0.4739\n",
      "Epoch 84/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.3446 - accuracy: 0.5169 - val_loss: 2.8997 - val_accuracy: 0.4744\n",
      "Epoch 85/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.3340 - accuracy: 0.5188 - val_loss: 2.8935 - val_accuracy: 0.4759\n",
      "Epoch 86/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.3240 - accuracy: 0.5204 - val_loss: 2.8889 - val_accuracy: 0.4769\n",
      "Epoch 87/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.3145 - accuracy: 0.5219 - val_loss: 2.8822 - val_accuracy: 0.4768\n",
      "Epoch 88/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.3048 - accuracy: 0.5232 - val_loss: 2.8783 - val_accuracy: 0.4786\n",
      "Epoch 89/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.2946 - accuracy: 0.5252 - val_loss: 2.8737 - val_accuracy: 0.4781\n",
      "Epoch 90/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.2890 - accuracy: 0.5261 - val_loss: 2.8685 - val_accuracy: 0.4808\n",
      "Epoch 91/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.2778 - accuracy: 0.5276 - val_loss: 2.8636 - val_accuracy: 0.4813\n",
      "Epoch 92/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.2670 - accuracy: 0.5297 - val_loss: 2.8566 - val_accuracy: 0.4833\n",
      "Epoch 93/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.2570 - accuracy: 0.5316 - val_loss: 2.8518 - val_accuracy: 0.4839\n",
      "Epoch 94/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.2482 - accuracy: 0.5329 - val_loss: 2.8481 - val_accuracy: 0.4857\n",
      "Epoch 95/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.2391 - accuracy: 0.5346 - val_loss: 2.8425 - val_accuracy: 0.4854\n",
      "Epoch 96/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.2306 - accuracy: 0.5360 - val_loss: 2.8379 - val_accuracy: 0.4866\n",
      "Epoch 97/200\n",
      "36/36 [==============================] - 50s 1s/step - loss: 2.2219 - accuracy: 0.5378 - val_loss: 2.8336 - val_accuracy: 0.4876\n",
      "Epoch 98/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 2.2149 - accuracy: 0.5388 - val_loss: 2.8288 - val_accuracy: 0.4879\n",
      "Epoch 99/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 2.2050 - accuracy: 0.5406 - val_loss: 2.8249 - val_accuracy: 0.4882\n",
      "Epoch 100/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 2.1960 - accuracy: 0.5413 - val_loss: 2.8210 - val_accuracy: 0.4900\n",
      "Epoch 101/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 2.1875 - accuracy: 0.5438 - val_loss: 2.8164 - val_accuracy: 0.4905\n",
      "Epoch 102/200\n",
      "36/36 [==============================] - 48s 1s/step - loss: 2.1800 - accuracy: 0.5446 - val_loss: 2.8111 - val_accuracy: 0.4927\n",
      "Epoch 103/200\n",
      "36/36 [==============================] - 48s 1s/step - loss: 2.1719 - accuracy: 0.5459 - val_loss: 2.8074 - val_accuracy: 0.4926\n",
      "Epoch 104/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 2.1637 - accuracy: 0.5473 - val_loss: 2.8039 - val_accuracy: 0.4922\n",
      "Epoch 105/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.1557 - accuracy: 0.5485 - val_loss: 2.7988 - val_accuracy: 0.4939\n",
      "Epoch 106/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.1478 - accuracy: 0.5501 - val_loss: 2.7959 - val_accuracy: 0.4948\n",
      "Epoch 107/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 2.1404 - accuracy: 0.5511 - val_loss: 2.7920 - val_accuracy: 0.4949\n",
      "Epoch 108/200\n",
      "36/36 [==============================] - 43s 1s/step - loss: 2.1334 - accuracy: 0.5529 - val_loss: 2.7876 - val_accuracy: 0.4950\n",
      "Epoch 109/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 2.1252 - accuracy: 0.5539 - val_loss: 2.7841 - val_accuracy: 0.4963\n",
      "Epoch 110/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 2.1176 - accuracy: 0.5557 - val_loss: 2.7810 - val_accuracy: 0.4966\n",
      "Epoch 111/200\n",
      "36/36 [==============================] - 48s 1s/step - loss: 2.1099 - accuracy: 0.5566 - val_loss: 2.7760 - val_accuracy: 0.4979\n",
      "Epoch 112/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 2.1025 - accuracy: 0.5576 - val_loss: 2.7742 - val_accuracy: 0.4977\n",
      "Epoch 113/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 2.0961 - accuracy: 0.5591 - val_loss: 2.7696 - val_accuracy: 0.4990\n",
      "Epoch 114/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 2.0921 - accuracy: 0.5594 - val_loss: 2.7663 - val_accuracy: 0.5004\n",
      "Epoch 115/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 2.0837 - accuracy: 0.5611 - val_loss: 2.7640 - val_accuracy: 0.5008\n",
      "Epoch 116/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 2.0753 - accuracy: 0.5630 - val_loss: 2.7583 - val_accuracy: 0.5013\n",
      "Epoch 117/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 2.0686 - accuracy: 0.5641 - val_loss: 2.7548 - val_accuracy: 0.5027\n",
      "Epoch 118/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.0613 - accuracy: 0.5651 - val_loss: 2.7514 - val_accuracy: 0.5028\n",
      "Epoch 119/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 2.0545 - accuracy: 0.5665 - val_loss: 2.7488 - val_accuracy: 0.5030\n",
      "Epoch 120/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.0470 - accuracy: 0.5683 - val_loss: 2.7450 - val_accuracy: 0.5051\n",
      "Epoch 121/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 2.0405 - accuracy: 0.5692 - val_loss: 2.7420 - val_accuracy: 0.5055\n",
      "Epoch 122/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.0344 - accuracy: 0.5701 - val_loss: 2.7404 - val_accuracy: 0.5063\n",
      "Epoch 123/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 2.0324 - accuracy: 0.5705 - val_loss: 2.7396 - val_accuracy: 0.5050\n",
      "Epoch 124/200\n",
      "36/36 [==============================] - 48s 1s/step - loss: 2.0238 - accuracy: 0.5720 - val_loss: 2.7332 - val_accuracy: 0.5069\n",
      "Epoch 125/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 2.0160 - accuracy: 0.5733 - val_loss: 2.7288 - val_accuracy: 0.5071\n",
      "Epoch 126/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 2.0100 - accuracy: 0.5746 - val_loss: 2.7262 - val_accuracy: 0.5087\n",
      "Epoch 127/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 2.0043 - accuracy: 0.5754 - val_loss: 2.7236 - val_accuracy: 0.5093\n",
      "Epoch 128/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.9970 - accuracy: 0.5771 - val_loss: 2.7236 - val_accuracy: 0.5088\n",
      "Epoch 129/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.9915 - accuracy: 0.5779 - val_loss: 2.7198 - val_accuracy: 0.5098\n",
      "Epoch 130/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.9854 - accuracy: 0.5791 - val_loss: 2.7153 - val_accuracy: 0.5104\n",
      "Epoch 131/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.9799 - accuracy: 0.5801 - val_loss: 2.7140 - val_accuracy: 0.5119\n",
      "Epoch 132/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.9736 - accuracy: 0.5814 - val_loss: 2.7108 - val_accuracy: 0.5127\n",
      "Epoch 133/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.9696 - accuracy: 0.5821 - val_loss: 2.7089 - val_accuracy: 0.5128\n",
      "Epoch 134/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 1.9631 - accuracy: 0.5832 - val_loss: 2.7064 - val_accuracy: 0.5126\n",
      "Epoch 135/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.9562 - accuracy: 0.5847 - val_loss: 2.7051 - val_accuracy: 0.5124\n",
      "Epoch 136/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 1.9508 - accuracy: 0.5857 - val_loss: 2.6998 - val_accuracy: 0.5144\n",
      "Epoch 137/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.9450 - accuracy: 0.5865 - val_loss: 2.6977 - val_accuracy: 0.5151\n",
      "Epoch 138/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.9394 - accuracy: 0.5877 - val_loss: 2.6959 - val_accuracy: 0.5172\n",
      "Epoch 139/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.9337 - accuracy: 0.5885 - val_loss: 2.6927 - val_accuracy: 0.5166\n",
      "Epoch 140/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.9301 - accuracy: 0.5895 - val_loss: 2.6944 - val_accuracy: 0.5167\n",
      "Epoch 141/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.9248 - accuracy: 0.5903 - val_loss: 2.6908 - val_accuracy: 0.5183\n",
      "Epoch 142/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.9188 - accuracy: 0.5916 - val_loss: 2.6871 - val_accuracy: 0.5172\n",
      "Epoch 143/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 1.9144 - accuracy: 0.5924 - val_loss: 2.6842 - val_accuracy: 0.5189\n",
      "Epoch 144/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.9092 - accuracy: 0.5929 - val_loss: 2.6800 - val_accuracy: 0.5199\n",
      "Epoch 145/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.9046 - accuracy: 0.5938 - val_loss: 2.6809 - val_accuracy: 0.5195\n",
      "Epoch 146/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.8992 - accuracy: 0.5953 - val_loss: 2.6764 - val_accuracy: 0.5209\n",
      "Epoch 147/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.8934 - accuracy: 0.5962 - val_loss: 2.6723 - val_accuracy: 0.5216\n",
      "Epoch 148/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.8881 - accuracy: 0.5974 - val_loss: 2.6732 - val_accuracy: 0.5216\n",
      "Epoch 149/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.8819 - accuracy: 0.5984 - val_loss: 2.6694 - val_accuracy: 0.5230\n",
      "Epoch 150/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.8776 - accuracy: 0.5991 - val_loss: 2.6692 - val_accuracy: 0.5229\n",
      "Epoch 151/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.8725 - accuracy: 0.6003 - val_loss: 2.6680 - val_accuracy: 0.5231\n",
      "Epoch 152/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.8675 - accuracy: 0.6012 - val_loss: 2.6620 - val_accuracy: 0.5251\n",
      "Epoch 153/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.8627 - accuracy: 0.6022 - val_loss: 2.6602 - val_accuracy: 0.5256\n",
      "Epoch 154/200\n",
      "36/36 [==============================] - 49s 1s/step - loss: 1.8581 - accuracy: 0.6031 - val_loss: 2.6597 - val_accuracy: 0.5253\n",
      "Epoch 155/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.8539 - accuracy: 0.6038 - val_loss: 2.6573 - val_accuracy: 0.5256\n",
      "Epoch 156/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.8484 - accuracy: 0.6046 - val_loss: 2.6563 - val_accuracy: 0.5258\n",
      "Epoch 157/200\n",
      "36/36 [==============================] - 48s 1s/step - loss: 1.8440 - accuracy: 0.6058 - val_loss: 2.6543 - val_accuracy: 0.5276\n",
      "Epoch 158/200\n",
      "36/36 [==============================] - 48s 1s/step - loss: 1.8392 - accuracy: 0.6066 - val_loss: 2.6524 - val_accuracy: 0.5272\n",
      "Epoch 159/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.8352 - accuracy: 0.6070 - val_loss: 2.6528 - val_accuracy: 0.5278\n",
      "Epoch 160/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.8330 - accuracy: 0.6075 - val_loss: 2.6528 - val_accuracy: 0.5276\n",
      "Epoch 161/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 1.8292 - accuracy: 0.6083 - val_loss: 2.6491 - val_accuracy: 0.5292\n",
      "Epoch 162/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 1.8240 - accuracy: 0.6092 - val_loss: 2.6506 - val_accuracy: 0.5284\n",
      "Epoch 163/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 1.8180 - accuracy: 0.6104 - val_loss: 2.6437 - val_accuracy: 0.5304\n",
      "Epoch 164/200\n",
      "36/36 [==============================] - 48s 1s/step - loss: 1.8130 - accuracy: 0.6118 - val_loss: 2.6415 - val_accuracy: 0.5302\n",
      "Epoch 165/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.8083 - accuracy: 0.6127 - val_loss: 2.6392 - val_accuracy: 0.5310\n",
      "Epoch 166/200\n",
      "36/36 [==============================] - 48s 1s/step - loss: 1.8052 - accuracy: 0.6132 - val_loss: 2.6407 - val_accuracy: 0.5304\n",
      "Epoch 167/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.8022 - accuracy: 0.6136 - val_loss: 2.6383 - val_accuracy: 0.5306\n",
      "Epoch 168/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7968 - accuracy: 0.6145 - val_loss: 2.6356 - val_accuracy: 0.5325\n",
      "Epoch 169/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7921 - accuracy: 0.6160 - val_loss: 2.6348 - val_accuracy: 0.5322\n",
      "Epoch 170/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.7880 - accuracy: 0.6167 - val_loss: 2.6330 - val_accuracy: 0.5338\n",
      "Epoch 171/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7832 - accuracy: 0.6172 - val_loss: 2.6304 - val_accuracy: 0.5342\n",
      "Epoch 172/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.7794 - accuracy: 0.6182 - val_loss: 2.6286 - val_accuracy: 0.5342\n",
      "Epoch 173/200\n",
      "36/36 [==============================] - 44s 1s/step - loss: 1.7753 - accuracy: 0.6191 - val_loss: 2.6274 - val_accuracy: 0.5341\n",
      "Epoch 174/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.7750 - accuracy: 0.6192 - val_loss: 2.6301 - val_accuracy: 0.5347\n",
      "Epoch 175/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.7703 - accuracy: 0.6202 - val_loss: 2.6252 - val_accuracy: 0.5354\n",
      "Epoch 176/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7654 - accuracy: 0.6211 - val_loss: 2.6229 - val_accuracy: 0.5358\n",
      "Epoch 177/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7602 - accuracy: 0.6221 - val_loss: 2.6213 - val_accuracy: 0.5358\n",
      "Epoch 178/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7554 - accuracy: 0.6230 - val_loss: 2.6199 - val_accuracy: 0.5375\n",
      "Epoch 179/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.7520 - accuracy: 0.6238 - val_loss: 2.6212 - val_accuracy: 0.5373\n",
      "Epoch 180/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7493 - accuracy: 0.6241 - val_loss: 2.6196 - val_accuracy: 0.5371\n",
      "Epoch 181/200\n",
      "36/36 [==============================] - 48s 1s/step - loss: 1.7456 - accuracy: 0.6251 - val_loss: 2.6155 - val_accuracy: 0.5370\n",
      "Epoch 182/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7432 - accuracy: 0.6254 - val_loss: 2.6210 - val_accuracy: 0.5364\n",
      "Epoch 183/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7419 - accuracy: 0.6252 - val_loss: 2.6134 - val_accuracy: 0.5382\n",
      "Epoch 184/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.7364 - accuracy: 0.6268 - val_loss: 2.6102 - val_accuracy: 0.5392\n",
      "Epoch 185/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7308 - accuracy: 0.6272 - val_loss: 2.6090 - val_accuracy: 0.5391\n",
      "Epoch 186/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7262 - accuracy: 0.6284 - val_loss: 2.6087 - val_accuracy: 0.5399\n",
      "Epoch 187/200\n",
      "36/36 [==============================] - 46s 1s/step - loss: 1.7225 - accuracy: 0.6290 - val_loss: 2.6069 - val_accuracy: 0.5402\n",
      "Epoch 188/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7186 - accuracy: 0.6299 - val_loss: 2.6060 - val_accuracy: 0.5405\n",
      "Epoch 189/200\n",
      "36/36 [==============================] - 49s 1s/step - loss: 1.7154 - accuracy: 0.6306 - val_loss: 2.6036 - val_accuracy: 0.5405\n",
      "Epoch 190/200\n",
      "36/36 [==============================] - 49s 1s/step - loss: 1.7121 - accuracy: 0.6315 - val_loss: 2.6034 - val_accuracy: 0.5415\n",
      "Epoch 191/200\n",
      "36/36 [==============================] - 48s 1s/step - loss: 1.7095 - accuracy: 0.6314 - val_loss: 2.6011 - val_accuracy: 0.5422\n",
      "Epoch 192/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7072 - accuracy: 0.6322 - val_loss: 2.6003 - val_accuracy: 0.5427\n",
      "Epoch 193/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.7025 - accuracy: 0.6327 - val_loss: 2.5976 - val_accuracy: 0.5433\n",
      "Epoch 194/200\n",
      "36/36 [==============================] - 49s 1s/step - loss: 1.6983 - accuracy: 0.6340 - val_loss: 2.5980 - val_accuracy: 0.5429\n",
      "Epoch 195/200\n",
      "36/36 [==============================] - 49s 1s/step - loss: 1.6947 - accuracy: 0.6342 - val_loss: 2.5962 - val_accuracy: 0.5434\n",
      "Epoch 196/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.6914 - accuracy: 0.6353 - val_loss: 2.5939 - val_accuracy: 0.5455\n",
      "Epoch 197/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.6875 - accuracy: 0.6359 - val_loss: 2.5936 - val_accuracy: 0.5452\n",
      "Epoch 198/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.6840 - accuracy: 0.6365 - val_loss: 2.5923 - val_accuracy: 0.5445\n",
      "Epoch 199/200\n",
      "36/36 [==============================] - 47s 1s/step - loss: 1.6805 - accuracy: 0.6372 - val_loss: 2.5903 - val_accuracy: 0.5455\n",
      "Epoch 200/200\n",
      "36/36 [==============================] - 45s 1s/step - loss: 1.6805 - accuracy: 0.6367 - val_loss: 2.5931 - val_accuracy: 0.5443\n",
      "WARNING:tensorflow:From /Users/thiennhan/Desktop/CMPU365_Project/env/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/thiennhan/Desktop/CMPU365_Project/env/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: s2s/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r = model.fit(\n",
    "    x=[padded_post_sequences, padded_comment_input_sequences],\n",
    "    y=decoder_targets_one_hot,\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "model.save(\"s2s\")\n",
    "#model.save_weights('saved_weights.hdf5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-3e6baa900c5c>, line 4)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-3e6baa900c5c>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    {'epochs': 200, 'latent_dim': 256, 'optimizer': 'adam', ''},\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Run overnight\n",
    "# Increase word embeddings to 200\n",
    "parameters = [\n",
    "    {'epochs': 200, 'latent_dim': 256, 'optimizer': 'adam', ''}, \n",
    "    {'epochs': 200, 'latent_dim': 256, 'optimizer': 'adam', ''}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"latent_dim_512_s2s\")\n",
    "model.summary()\n",
    "# for idx, layer in enumerate(model.layers):\n",
    "#     print(idx, layer.name)\n",
    "# return\n",
    "latent_dim = 256\n",
    "\"\"\"\n",
    "encoder_inputs = Input(shape=(AVG_POST_LEN,)) # shape=(None,), dtype=\"int64\"\n",
    "enc_emb = embedding_layer(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(AVG_COMMENT_LEN,)) \n",
    "dec_emb_layer = Embedding(vocab_size+1, latent_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "# for i in range(6):\n",
    "#     print(i)\n",
    "#     print(model.layers[i].output)\n",
    "# embedding_layer = model.layers[2]\n",
    "# enc_emb = embedding_layer(encoder_inputs)\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[4].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "\n",
    "# for i in range(4):\n",
    "#     print(model.input[i])\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "dec_emb_layer = model.layers[3]\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = model.layers[5]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    dec_emb, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[6]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "from keras.utils import plot_model\n",
    "plot_model(encoder_model, to_file='../LSTM/encoder_inference_model.png', show_shapes=True, show_layer_names=True)\n",
    "plot_model(decoder_model, to_file='../LSTM/decoder_inference_model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "def give_encouragement(input_seq):\n",
    "    #Marathi one\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = word_to_index['<START>']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_word[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '<END>' or len(decoded_sentence) > AVG_COMMENT_LEN):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for _ in range(5):\n",
    "# for seq_index in range(2):\n",
    "#     # Take one sequence (part of the training set)\n",
    "#     # for trying out decoding.\n",
    "#     input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "for _ in range(5):\n",
    "    i = np.random.choice(len(posts))\n",
    "    input_seq = padded_post_sequences[i:i+1]\n",
    "    generated_comment = give_encouragement(input_seq)\n",
    "    print('-')\n",
    "    print('Post: ', posts[i])\n",
    "    print('Generated Comment: ', generated_comment)\n",
    "    print(\"Actual Comment: \", comments_input[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(encoder_inputs.shape, encoder_states[0].shape, encoder_states[1].shape)\n",
    "# print(decoder_state_input_h.shape)\n",
    "# print(decoder_state_input_c.shape)\n",
    "# print(decoder_inputs_single.shape)\n",
    "# print(decoder_outputs.shape)\n",
    "# for i in decoder_states_inputs:\n",
    "#     print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "# # Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "plot_model(encoder_model, to_file='../LSTM/model_plot_enc.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = dec_emb_layer(decoder_inputs_single)\n",
    "\n",
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# # To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "# decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "# decoder_states2 = [state_h2, state_c2]\n",
    "# decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# # Encode the input sequence to get the \"thought vectors\"\n",
    "# encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# # Decoder setup\n",
    "# # Below tensors will hold the states of the previous time step\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# # To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "# decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "# decoder_states2 = [state_h2, state_c2]\n",
    "# decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# # Final decoder model\n",
    "# decoder_model = Model(\n",
    "#     [decoder_inputs] + decoder_states_inputs,\n",
    "#     [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "\n",
    "plot_model(decoder_model, to_file='../LSTM/model_plot_dec.png', show_shapes=True, show_layer_names=True)\n",
    "encoder_model.save_weights('encoder_model_weights.hdf5', overwrite=True)\n",
    "decoder_model.save_weights('decoder_model_weights.hdf5', overwrite=True)"
   ]
  },
  {
   "source": [
    "Keras LSTM Architecture\n",
    "\n",
    "The input shape of the text data is ordered as follows: batch size, number of time steps, hidden size (size of the hidden layer)\n",
    "For each batch sample and each word in the number of time steps, there is a l500 length embedding word vector to "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_model.load_weights(\"../LSTM/encoder_model_weights.hdf5\")\n",
    "    # decoder_model.load_weights(\"../LSTM/decoder_model_weights.hdf5\")\n",
    "    \n",
    "    # # KERAS\n",
    "    # states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # # Generate empty target sequence of length 1.\n",
    "    # target_seq = np.zeros((1, 1, vocab_size))\n",
    "    # # Populate the first character of target sequence with the start character.\n",
    "    # target_seq[0, 0, word_to_index['<START>']] = 1.0\n",
    "\n",
    "    # # Sampling loop for a batch of sequences\n",
    "    # # (to simplify, here we assume a batch of size 1).\n",
    "    # stop_condition = False\n",
    "    # decoded_sentence = \"\"\n",
    "    # while not stop_condition:\n",
    "    #     output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "    #     # Sample a token\n",
    "    #     sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    #     sampled_char = index_to_word[sampled_token_index]\n",
    "    #     decoded_sentence += sampled_char\n",
    "\n",
    "    #     # Exit condition: either hit max length\n",
    "    #     # or find stop character.\n",
    "    #     if sampled_char == \"<END>\" or len(decoded_sentence) > AVG_COMMENT_LEN:\n",
    "    #         stop_condition = True\n",
    "\n",
    "    #     # Update the target sequence (of length 1).\n",
    "    #     target_seq = np.zeros((1, 1, word_to_index['<START>']))\n",
    "    #     target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "    #     # Update states\n",
    "    #     states_value = [h, c]\n",
    "    # return decoded_sentence\n",
    "\n",
    "    # # The french one\n",
    "    # print(input_seq.shape)\n",
    "    # states_value = encoder_model.predict(input_seq)\n",
    "    # target_seq = np.zeros((1, 1))\n",
    "    # target_seq[0, 0] = word_to_index['<START>']\n",
    "    # eos = word_to_index['<END>']\n",
    "    # output_sentence = []\n",
    "    # for _ in range(AVG_COMMENT_LEN):\n",
    "    #     # print(states_value)\n",
    "    #     # print(target_seq.shape)\n",
    "    #     # inputs = [target_seq] + states_value\n",
    "    #     # print(inputs)\n",
    "    #     # print(states_value)\n",
    "    #     # for i in range(len(states_value)):\n",
    "    #     #     states_value[i] = np.asarray(states_value[i])\n",
    "    #     # print(states_value)\n",
    "    #     # temp = [target_seq]\n",
    "    #     # print(temp)\n",
    "    #     # temp.append(states_value)\n",
    "    #     # for i in [target_seq] + states_value:\n",
    "    #         # print(i.shape)\n",
    "    #     output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    #     idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "    #     if eos == idx:\n",
    "    #         break\n",
    "\n",
    "    #     word = ''\n",
    "\n",
    "    #     if idx > 0:\n",
    "    #         word = index_to_word[idx]\n",
    "    #         output_sentence.append(word)\n",
    "\n",
    "    #     target_seq[0, 0] = idx\n",
    "    #     states_value = [h, c]\n",
    "\n",
    "    # return ' '.join(output_sentence)\n",
    "\n",
    "    # The weird one\n",
    "    # #Getting the output states to pass into the decoder\n",
    "    # states_value = encoder_model.predict(input_seq)\n",
    "    # #Generating empty target sequence of length 1\n",
    "    # target_seq = np.zeros((1, 1, vocab_size+1))\n",
    "    # #Setting the first token of target sequence with the start token\n",
    "    # target_seq[0, 0, word_to_index['<START>']] = 1.\n",
    "    \n",
    "    # #A variable to store our response word by word\n",
    "    # decoded_sentence = ''\n",
    "    \n",
    "    # stop_condition = False\n",
    "    # while not stop_condition:\n",
    "    #   #Predicting output tokens with probabilities and states\n",
    "    #   output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq] + states_value)\n",
    "    #   #Choosing the one with highest probability\n",
    "    #   sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    #   sampled_token = index_to_word[sampled_token_index]\n",
    "    #   decoded_sentence += \" \" + sampled_token#Stop if hit max length or found the stop token\n",
    "    #   if (sampled_token == '<END>' or len(decoded_sentence) > AVG_COMMENT_LEN):\n",
    "    #       stop_condition = True\n",
    "    #   #Update the target sequence\n",
    "    #   target_seq = np.zeros((1, 1, vocab_size+1))\n",
    "    #   target_seq[0, 0, sampled_token_index] = 1.\n",
    "    #   #Update states\n",
    "    #   states_value = [hidden_state, cell_state]\n",
    "    # return decoded_sentence"
   ]
  }
 ]
}