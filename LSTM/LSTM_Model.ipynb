{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "f2f75a0ebfdd6e2b92c174594ae097d40e0e557cdf4b7d9b70ffaf179ea13763"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h1>LSTM Model for Encouragement Generator</h1>\n",
    "CMPU 365\n",
    "Jason Lee, Nhan Nguyen\n",
    "\n",
    "We have consulted and adapted code from the following sources in the making of this model: \n",
    "- https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/\n",
    "- https://keras.io/examples/nlp/lstm_seq2seq/#run-inference-sampling\n",
    "- https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "import csv\n",
    "from CleanText import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_method = \"one-hot\"\n",
    "# Use \"embed\" for word embedding output"
   ]
  },
  {
   "source": [
    "<h2>Text Preprocessing</h2>\n",
    "clean_text function is adapted from https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_char = \"<START>\"\n",
    "end_char = \"<END>\"\n",
    "\n",
    "posts = []\n",
    "comments_output = []\n",
    "comments_input = []\n",
    "\n",
    "with open('../splitted_data_3.csv', 'r', newline='') as csv_file:\n",
    "    textReader = csv.reader(csv_file)\n",
    "    for row in textReader:\n",
    "        # Each row in the csv_file is of the form Comment, Post\n",
    "        posts.append(\" \".join(clean_text(row[1])))\n",
    "        cleaned_text = clean_text(row[0])\n",
    "        cleaned_text = [start_char] + cleaned_text + [end_char]\n",
    "        comments_output.append(\" \".join(cleaned_text[1:]))\n",
    "        comments_input.append(\" \".join(cleaned_text[:-1]))"
   ]
  },
  {
   "source": [
    "<h3>Tokenize the sentences</h3>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Generate all unique words in the dataset\n",
    "all_words = set()\n",
    "all_text = posts + comments_output + comments_input\n",
    "for sentence in all_text:\n",
    "    for word in sentence.split():\n",
    "        all_words.add(word)\n",
    "\n",
    "# Tokenize the sentences\n",
    "vocab_size = len(all_words)\n",
    "tokenizer = Tokenizer(num_words = vocab_size+1, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "word_to_index = tokenizer.word_index\n",
    "with open(\"word_to_index.csv\", \"w\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"word\", \"index\"])\n",
    "    writer.writeheader()\n",
    "    for k,v in word_to_index.items():\n",
    "        writer.writerow({\"word\": k, \"index\":v})\n",
    "index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "posts_sequence = tokenizer.texts_to_sequences(posts)\n",
    "comments_output_sequence = tokenizer.texts_to_sequences(comments_output)\n",
    "comments_input_sequence = tokenizer.texts_to_sequences(comments_input)\n"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "source": [
    "<h3>Pad and Truncate Sentences</h3>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "251\n"
     ]
    }
   ],
   "source": [
    "max_post_len = max(len(seq) for seq in posts_sequence)\n",
    "median_post_length = np.median([len(seq) for seq in posts_sequence])\n",
    "post_len = int(round((max_post_len+median_post_length)/2))\n",
    "padded_post_sequences = pad_sequences(posts_sequence, maxlen=post_len, truncating='post')\n",
    "print(post_len)\n",
    "\n",
    "avg_comment_len = np.average([len(seq) for seq in comments_output_sequence])\n",
    "max_comment_len = max([len(seq) for seq in comments_output_sequence])\n",
    "comment_len = int(round((max_comment_len+avg_comment_len)/2))\n",
    "comments_input_sequence = [\n",
    "    x if len(x) <= comment_len else x[:comment_len] for x in comments_input_sequence\n",
    "]\n",
    "padded_comment_input_sequences = pad_sequences(comments_input_sequence, maxlen=comment_len, padding='post')\n",
    "comments_output_sequence = [\n",
    "    x if len(x) <= comment_len else x[:comment_len-1]+[word_to_index[end_char]] for x in comments_output_sequence\n",
    "]\n",
    "padded_comment_output_sequences = pad_sequences(comments_output_sequence, maxlen=comment_len, padding='post')"
   ]
  },
  {
   "source": [
    "<h2>Build Model</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3>Embedding Layer</h3>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer\n",
    "embeddings_dictionary = {}\n",
    "with open('./glove6B/glove.6B.200d.txt', 'r') as glove:\n",
    "    for line in glove:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "        \n",
    "embeddings_tokenized = np.zeros((vocab_size+1, embedding_dim))\n",
    "if decoder_method == \"one-hot\":\n",
    "    embedding_dim = 200\n",
    "    for word, i in word_to_index.items():\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embeddings_tokenized[i] = embedding_vector\n",
    "    embeddings_file_name = \"embeddings_tokenized_one-hot.txt\"\n",
    "else:\n",
    "    embedding_dim = 200+3\n",
    "    for word, i in word_to_index.items():\n",
    "        if word == start_char:\n",
    "            embeddings_tokenized[i] = np.append(np.zeros((1, embedding_dim-3)), [1,0,0])\n",
    "        elif word == end_char:\n",
    "            embeddings_tokenized[i] = np.append(np.zeros((1, embedding_dim-3)), [0,1,0])\n",
    "        else:\n",
    "            embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embeddings_tokenized[i] = np.append(embedding_vector, [0,0,0])\n",
    "    embeddings_file_name = \"embeddings_tokenized_embed.txt\"\n",
    "\n",
    "    embeddings_tokenized[0] = np.append(np.zeros((1, embedding_dim-3)), [0,0,1])\n",
    "\n",
    "\n",
    "with open(embeddings_file, \"wb\") as embeddings_file:\n",
    "    np.save(embeddings_file_name, embeddings_tokenized)\n",
    "embedding_layer = Embedding(vocab_size+1, embedding_dim, embeddings_initializer=Constant(embeddings_tokenized), input_length=post_len, trainable=False)"
   ]
  },
  {
   "source": [
    "<h3>Decoder Output</h3>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if decoder_method == \"one-hot\":\n",
    "    decoder_targets = np.zeros((\n",
    "        len(posts),\n",
    "        comment_len,\n",
    "        vocab_size+1\n",
    "        ), \n",
    "        dtype='float32' \n",
    "    )\n",
    "    # One-hot encoding of the output\n",
    "    for i, sequences in enumerate(padded_comment_output_sequences):\n",
    "        for j, seq in enumerate(sequences):\n",
    "            decoder_targets[i, j, seq] = 1\n",
    "else:\n",
    "    decoder_targets = np.zeros((\n",
    "        len(posts), \n",
    "        comment_len, \n",
    "        embedding_dim\n",
    "        ), \n",
    "        dtype='float32'\n",
    "    )\n",
    "    for i, seqs in enumerate(padded_comment_output_sequences):\n",
    "        for j, seq in enumerate(seqs):\n",
    "            decoder_targets[i, j] = embeddings_tokenized[seq]"
   ]
  },
  {
   "source": [
    "<h3>Construct Network</h3>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "if decoder_method == \"one-hot\":\n",
    "    epochs = 500\n",
    "    latent_dim = 512\n",
    "    optimizer = \"rmsprop\"\n",
    "    activation = \"softmax\"\n",
    "    loss_function = \"categorical_crossentropy\"\n",
    "else:\n",
    "    epochs = 800\n",
    "    latent_dim = 200\n",
    "    optimizer = \"rmsprop\"\n",
    "    loss_function = \"cosine_similarity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the encoder\n",
    "encoder_inputs = Input(shape=(post_len,))\n",
    "enc_emb = embedding_layer(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "# Discard text output\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the decoder\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(comment_len,)) \n",
    "# Different embedding layer depending on decoder method\n",
    "if decoder_method == \"one-hot\":\n",
    "    dec_emb_layer = Embedding(vocab_size+1, latent_dim)\n",
    "else:\n",
    "    dec_emb_layer = Embedding(vocab_size+1, embedding_dim, embeddings_initializer=Constant(embeddings_tokenized), input_length=comment_len, trainable=False)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "# Discard output for inner states\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if decoder_method == \"one-hot\":\n",
    "    # Probability distribution over all words in vocabulary\n",
    "    decoder_dense = Dense(vocab_size+1, activation=activation)\n",
    "else: \n",
    "    # Produce individual dimensions in embedding\n",
    "    decoder_dense = Dense(embedding_dim)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "source": [
    "<h3>Fit model to data</h3>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_function,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "r = model.fit(\n",
    "    x=[padded_post_sequences, padded_comment_input_sequences],\n",
    "    y=decoder_targets,\n",
    "    batch_size=1,\n",
    "    epochs=1,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "model_name = \"LSTM_One_Hot\" if decoder_method == \"one-hot\" else \"LSTM_Embed\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='../LSTM/model.png', show_shapes=True, show_layer_names=True)"
   ]
  }
 ]
}