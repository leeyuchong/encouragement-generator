{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h1>LSTM Inference Model for Encouragement Generator</h1>\n",
    "CMPU 365\n",
    "Jason Lee, Nhan Nguyen\n",
    "\n",
    "We have consulted and adapted code from the following sources in the making of this model: \n",
    "- https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/\n",
    "- https://keras.io/examples/nlp/lstm_seq2seq/#run-inference-sampling\n",
    "- https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from keras.losses import cosine_similarity\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import csv\n",
    "from CleanText import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_method = \"one-hot\"\n",
    "# Use \"embed\" for word embedding output"
   ]
  },
  {
   "source": [
    "<h3>Load constsnts</h3>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_char = \"<START>\"\n",
    "end_char = \"<END>\"\n",
    "post_len = 251\n",
    "comment_len = 116\n",
    "word_to_index = {}\n",
    "with open(\"word_to_index.csv\", 'r', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader) # toss headers\n",
    "    for word, index in reader:\n",
    "        word_to_index.setdefault(word, int(index))\n",
    "index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "\n",
    "embeddings_file_name = \"embeddings_tokenized_one-hot.txt\" if decoder_method == \"one-hot\" else \"embeddings_tokenized_embed\"\n",
    "\n",
    "with open(embeddings_file_name, \"rb\") as embeddings_file:\n",
    "    embeddings_tokenized = np.load(embeddings_file)"
   ]
  },
  {
   "source": [
    "<h3>Construct Inference Model</h3>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LSTM_one-hot_model\" if decoder_method == \"one-hot\" else \"LSTM_Embed_mode\"\n",
    "model = load_model(model_name)\n",
    "# Get the latent_dim from the model\n",
    "latent_dim = model.layers[4].output[0].shape[1]\n",
    "\n",
    "# Create encoder\n",
    "encoder_inputs = model.input[0] #input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[4].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Create decoder\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = Input(shape=(latent_dim,), name=\"input_3\")\n",
    "decoder_state_input_c = Input(shape=(latent_dim,), name=\"input_4\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "dec_emb_layer = model.layers[3]\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = model.layers[5]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    dec_emb, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[6]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_encouragement(input_text):\n",
    "    \"\"\"\n",
    "    Use the model to produce output given an input sequence\n",
    "    Input: input_text, input that the model will generate text for\n",
    "    Output: a string representing the output of the model \n",
    "    \"\"\"\n",
    "    # Convert string to padded sequence of integers\n",
    "    input_sequence = clean_text(input_text)\n",
    "    input_sequence = [word_to_index[x] for x in input_sequence]\n",
    "    input_sequence = pad_sequences([input_sequence], maxlen=post_len, truncating='post')\n",
    "    # Get internal state of encoder\n",
    "    states_value = encoder_model.predict(input_sequence)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Start output with the start symbol\n",
    "    target_seq[0, 0] = word_to_index[start_char]\n",
    "    stop_condition = False\n",
    "    output = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h_state, c_state = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_word = \"\"\n",
    "        sampled_token_index = 0\n",
    "        if decoder_method == \"one-hot\":\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_word = index_to_word[sampled_token_index]\n",
    "        else:\n",
    "            similarity = math.inf\n",
    "            for i in range(embeddings_tokenized.shape[0]):\n",
    "                new_sim = cosine_similarity(embeddings_tokenized[i:i+1].astype('float32'), output_tokens[0, 0].astype('float32'), axis=-1)\n",
    "                if new_sim < similarity:\n",
    "                    similarity = new_sim\n",
    "                    sampled_token_index = i\n",
    "            sampled_word = index_to_word[sampled_token_index]\n",
    "        output += ' '+sampled_word\n",
    "        if (sampled_word==end_char or len(output)>comment_len):\n",
    "            stop_condition = True\n",
    "        # Reset target_seq, pass current states forward\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h_state, c_state]\n",
    "    return output"
   ]
  }
 ]
}